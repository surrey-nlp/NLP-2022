{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 - Topic Modelling\n",
    "In this lab we will look into building topic models, but will also examine dimensionality reduction and other relevant subjects.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDiA)\n",
    "\n",
    "Based on: [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "\n",
    "For this lab, we’ll use the dataset of papers published in NeurIPS (NIPS) conference. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016. These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "** **\n",
    "#### Step 1: Loading Data\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 12:34:24--  https://www.cse.iitb.ac.in/~diptesh/data.tar.gz\n",
      "Resolving www.cse.iitb.ac.in (www.cse.iitb.ac.in)... 103.21.127.134\n",
      "Connecting to www.cse.iitb.ac.in (www.cse.iitb.ac.in)|103.21.127.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 73485589 (70M) [application/x-gzip]\n",
      "Saving to: 'data.tar.gz'\n",
      "\n",
      "data.tar.gz         100%[===================>]  70.08M  12.3MB/s    in 7.4s    \n",
      "\n",
      "2022-03-01 12:34:32 (9.49 MB/s) - 'data.tar.gz' saved [73485589/73485589]\n",
      "\n",
      "data/\n",
      "data/papers.csv\n",
      "data/.DS_Store\n",
      "data/articles+4.txt\n",
      "data/authors.csv\n",
      "data/paper_authors.csv\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.cse.iitb.ac.in/~diptesh/data.tar.gz && tar -xvzf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#os.chdir('..')\n",
    "\n",
    "# Read data into papers\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 2: Data Cleaning\n",
    "** **\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
       "      <td>830\\n\\nInvariant Object Recognition Using a Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>338\\n\\nThe Connectivity Analysis of Simple Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6206</th>\n",
       "      <td>An Information-Theoretic Approach to\\nDecipher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>Mind the Nuisance: Gaussian Process\\nClassific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>Integrate-and-Fire models with adaptation are\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paper_text\n",
       "6355  830\\n\\nInvariant Object Recognition Using a Di...\n",
       "4748  338\\n\\nThe Connectivity Analysis of Simple Ass...\n",
       "6206  An Information-Theoretic Approach to\\nDecipher...\n",
       "4830  Mind the Nuisance: Gaussian Process\\nClassific...\n",
       "2045  Integrate-and-Fire models with adaptation are\\..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'title', 'abstract', \n",
    "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
    "\n",
    "# sample only 100 papers\n",
    "papers = papers.sample(100)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6355    830\\n\\ninvariant object recognition using a di...\n",
       "4748    338\\n\\nthe connectivity analysis of simple ass...\n",
       "6206    an information-theoretic approach to\\ndecipher...\n",
       "4830    mind the nuisance: gaussian process\\nclassific...\n",
       "2045    integrate-and-fire models with adaptation are\\...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize words and further clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['invariant', 'object', 'recognition', 'using', 'distributed', 'associative', 'memory', 'harry', 'wechsler', 'and', 'george', 'lee', 'zimmerman', 'department', 'or', 'electrical', 'engineering', 'university', 'or', 'minnesota', 'minneapolis', 'mn', 'abstract', 'this', 'paper', 'describes', 'an', 'approach', 'to', 'dimensional']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "** **\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 s, sys: 59.4 ms, total: 5.36 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vrusiasb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the functions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pathy in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.58.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.17.19)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.19 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.19)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.19->boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.19->boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['invariant', 'object', 'recognition', 'use', 'distributed_associative', 'memory', 'harry', 'wechsl', 'george', 'lee', 'zimmerman', 'department', 'electrical_engineering', 'university', 'minnesota', 'minneapolis', 'abstract', 'paper', 'describe', 'approach', 'dimensional', 'object', 'recognition', 'complex', 'log', 'conformal', 'mapping', 'combine', 'distributed_associative', 'memory']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 4: Data transformation: Corpus and Dictionary\n",
    "** **\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 4), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 3), (16, 4), (17, 1), (18, 5), (19, 2), (20, 4), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 4), (27, 2), (28, 1), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 5: Base Model \n",
    "** **\n",
    "\n",
    "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
    "\n",
    "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 3.35 s, total: 26 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.013*\"model\" + 0.009*\"set\" + 0.008*\"image\" + 0.008*\"use\" + 0.006*\"datum\" + '\n",
      "  '0.006*\"face\" + 0.005*\"function\" + 0.005*\"learn\" + 0.005*\"time\" + '\n",
      "  '0.005*\"algorithm\"'),\n",
      " (1,\n",
      "  '0.011*\"model\" + 0.009*\"sample\" + 0.009*\"distribution\" + 0.008*\"use\" + '\n",
      "  '0.008*\"test\" + 0.007*\"datum\" + 0.007*\"set\" + 0.006*\"function\" + '\n",
      "  '0.006*\"result\" + 0.006*\"point\"'),\n",
      " (2,\n",
      "  '0.013*\"model\" + 0.009*\"use\" + 0.008*\"state\" + 0.007*\"learn\" + 0.007*\"time\" '\n",
      "  '+ 0.006*\"datum\" + 0.006*\"function\" + 0.006*\"policy\" + 0.006*\"show\" + '\n",
      "  '0.005*\"local\"'),\n",
      " (3,\n",
      "  '0.011*\"function\" + 0.009*\"algorithm\" + 0.008*\"use\" + 0.007*\"set\" + '\n",
      "  '0.007*\"sample\" + 0.007*\"error\" + 0.006*\"learn\" + 0.006*\"problem\" + '\n",
      "  '0.006*\"theorem\" + 0.006*\"distribution\"'),\n",
      " (4,\n",
      "  '0.009*\"distribution\" + 0.008*\"function\" + 0.008*\"learn\" + 0.007*\"method\" + '\n",
      "  '0.007*\"feature\" + 0.006*\"show\" + 0.006*\"result\" + 0.006*\"set\" + '\n",
      "  '0.005*\"problem\" + 0.005*\"variable\"'),\n",
      " (5,\n",
      "  '0.015*\"function\" + 0.012*\"network\" + 0.011*\"algorithm\" + 0.007*\"input\" + '\n",
      "  '0.007*\"set\" + 0.007*\"time\" + 0.006*\"use\" + 0.006*\"learn\" + 0.006*\"weight\" + '\n",
      "  '0.005*\"learning\"'),\n",
      " (6,\n",
      "  '0.009*\"algorithm\" + 0.008*\"model\" + 0.008*\"learn\" + 0.007*\"matrix\" + '\n",
      "  '0.007*\"use\" + 0.007*\"learning\" + 0.006*\"datum\" + 0.006*\"method\" + '\n",
      "  '0.006*\"function\" + 0.006*\"show\"'),\n",
      " (7,\n",
      "  '0.010*\"model\" + 0.010*\"use\" + 0.008*\"datum\" + 0.007*\"word\" + 0.007*\"set\" + '\n",
      "  '0.006*\"object\" + 0.005*\"input\" + 0.005*\"network\" + 0.005*\"time\" + '\n",
      "  '0.005*\"class\"'),\n",
      " (8,\n",
      "  '0.010*\"algorithm\" + 0.009*\"problem\" + 0.007*\"use\" + 0.006*\"set\" + '\n",
      "  '0.006*\"model\" + 0.006*\"result\" + 0.005*\"variable\" + 0.005*\"graph\" + '\n",
      "  '0.005*\"follow\" + 0.005*\"matrix\"'),\n",
      " (9,\n",
      "  '0.009*\"image\" + 0.007*\"human\" + 0.006*\"use\" + 0.005*\"datum\" + '\n",
      "  '0.005*\"function\" + 0.005*\"model\" + 0.005*\"population\" + 0.005*\"search\" + '\n",
      "  '0.005*\"time\" + 0.005*\"algorithm\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Let's calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.27588331858083587\n",
      "CPU times: user 582 ms, sys: 138 ms, total: 720 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 6: Hyperparameter tuning\n",
    "#### Do not do that during the labs as it will take a long time! So skip to Step 7 for now\n",
    "** **\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters: \n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "- Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 151/540 [39:55<1:57:47, 18.17s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('../results/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 7: Final Model\n",
    "** **\n",
    "\n",
    "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*\"model\" + 0.008*\"use\" + 0.007*\"datum\" + 0.007*\"set\" + '\n",
      "  '0.007*\"function\" + 0.007*\"image\" + 0.007*\"learn\" + 0.005*\"result\" + '\n",
      "  '0.005*\"class\" + 0.005*\"learning\"'),\n",
      " (1,\n",
      "  '0.005*\"function\" + 0.005*\"model\" + 0.005*\"distribution\" + 0.004*\"result\" + '\n",
      "  '0.004*\"datum\" + 0.004*\"sample\" + 0.004*\"use\" + 0.003*\"algorithm\" + '\n",
      "  '0.003*\"method\" + 0.003*\"show\"'),\n",
      " (2,\n",
      "  '0.011*\"model\" + 0.007*\"use\" + 0.006*\"time\" + 0.006*\"state\" + '\n",
      "  '0.006*\"distribution\" + 0.006*\"algorithm\" + 0.005*\"policy\" + 0.005*\"cell\" + '\n",
      "  '0.005*\"function\" + 0.005*\"learn\"'),\n",
      " (3,\n",
      "  '0.008*\"algorithm\" + 0.006*\"function\" + 0.006*\"problem\" + 0.005*\"use\" + '\n",
      "  '0.005*\"sample\" + 0.004*\"learn\" + 0.004*\"model\" + 0.004*\"set\" + '\n",
      "  '0.004*\"error\" + 0.004*\"vector\"'),\n",
      " (4,\n",
      "  '0.004*\"crf\" + 0.004*\"relaxation\" + 0.003*\"message\" + 0.003*\"variable\" + '\n",
      "  '0.002*\"potential\" + 0.002*\"inference\" + 0.002*\"define\" + 0.002*\"factor\" + '\n",
      "  '0.002*\"estimation\" + 0.002*\"message_estimator\"'),\n",
      " (5,\n",
      "  '0.009*\"network\" + 0.008*\"function\" + 0.007*\"algorithm\" + 0.005*\"input\" + '\n",
      "  '0.005*\"set\" + 0.005*\"weight\" + 0.004*\"learn\" + 0.004*\"time\" + 0.004*\"use\" + '\n",
      "  '0.004*\"error\"'),\n",
      " (6,\n",
      "  '0.004*\"object\" + 0.004*\"image\" + 0.004*\"matrix\" + 0.004*\"result\" + '\n",
      "  '0.004*\"trajectory\" + 0.003*\"vector\" + 0.003*\"use\" + 0.003*\"surrogate\" + '\n",
      "  '0.003*\"show\" + 0.003*\"loss\"'),\n",
      " (7,\n",
      "  '0.007*\"set\" + 0.005*\"word\" + 0.005*\"model\" + 0.004*\"use\" + 0.004*\"time\" + '\n",
      "  '0.004*\"human\" + 0.003*\"query\" + 0.003*\"matrix\" + 0.003*\"response\" + '\n",
      "  '0.003*\"non\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 8: Visualize Results\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp2021-2/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el99371402293504234567435195691\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el99371402293504234567435195691_data = {\"mdsDat\": {\"x\": [0.08763888992752546, 0.05519167909526052, 0.0261246841678926, 0.01827934955888325, 0.004910183149798353, -0.03466633953117678, -0.04551598103012285, -0.11196246533806059], \"y\": [0.026778662835475885, -0.027986538783751954, 0.004406270583017532, -0.05316808046740963, 0.01798689816302609, 0.04460756946089433, 0.0014913922901635959, -0.014116174081415883], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [31.39300509209407, 20.235449910069946, 12.957075673899284, 12.461287227300518, 11.176085270951287, 5.532799097572287, 4.619841700292238, 1.624456027820362]}, \"tinfo\": {\"Term\": [\"network\", \"image\", \"object\", \"policy\", \"cell\", \"set\", \"word\", \"variable\", \"face\", \"trajectory\", \"spike\", \"query\", \"human\", \"matrix\", \"time\", \"algorithm\", \"layer\", \"vector\", \"neuron\", \"potential\", \"inference\", \"loss\", \"response\", \"node\", \"relaxation\", \"sample\", \"feature\", \"estimation\", \"label\", \"distribution\", \"pose\", \"caption\", \"bird\", \"stack\", \"keypoint\", \"sne\", \"adaboost\", \"boost\", \"pylon\", \"person\", \"neighborhood\", \"null\", \"verb\", \"card\", \"rollout\", \"deque\", \"name\", \"arc\", \"face\", \"appearance\", \"margin\", \"lsh\", \"base_learner\", \"assemble\", \"pij\", \"visualization\", \"statistical_test\", \"pool\", \"embed\", \"beak\", \"cvpr\", \"image\", \"label\", \"text\", \"nearest_neighbor\", \"class\", \"local\", \"training\", \"test\", \"train\", \"classification\", \"segmentation\", \"object\", \"classifier\", \"datum\", \"learning\", \"model\", \"set\", \"learn\", \"task\", \"use\", \"point\", \"result\", \"function\", \"example\", \"follow\", \"base\", \"method\", \"show\", \"problem\", \"distribution\", \"algorithm\", \"error\", \"state\", \"give\", \"figure\", \"number\", \"time\", \"fvs\", \"pddp\", \"imitation\", \"attractor\", \"coalescent\", \"gcld\", \"psdp\", \"pld\", \"rmgpc\", \"ddp\", \"pilco\", \"reservoir\", \"spike\", \"fire\", \"maze\", \"fvss\", \"sampler\", \"teacher\", \"stationary\", \"coughlan\", \"phoneme\", \"recording\", \"genealogy\", \"actor_tutor\", \"cart\", \"rhythmic\", \"cps\", \"hmc\", \"bfgs\", \"dispersion\", \"cell\", \"expert\", \"policy\", \"chain\", \"neuron\", \"control\", \"goal\", \"state\", \"bayesian\", \"reach\", \"path\", \"nonlinear\", \"time\", \"model\", \"trajectory\", \"distribution\", \"dynamic\", \"system\", \"use\", \"latent\", \"algorithm\", \"action\", \"figure\", \"learn\", \"show\", \"information\", \"datum\", \"give\", \"function\", \"sample\", \"probability\", \"number\", \"gaussian\", \"parameter\", \"value\", \"set\", \"base\", \"problem\", \"method\", \"result\", \"codon\", \"tier\", \"dropout\", \"gpc\", \"cumulant\", \"lda_moment\", \"privileged_information\", \"aew\", \"kavg\", \"mcv\", \"xdt\", \"privileged\", \"hgf\", \"ea\", \"slg\", \"zd\", \"lle\", \"submodular_minimization\", \"yqt\", \"regularized_distance\", \"alphanet\", \"rq\", \"lnp\", \"cdn\", \"rdn\", \"lda\", \"mono\", \"divergence_estimate\", \"ad\", \"greedy_importance\", \"topic\", \"mutual_information\", \"divergence\", \"submodular\", \"document\", \"appendix\", \"algorithm\", \"problem\", \"sample\", \"solution\", \"hmm\", \"graph\", \"vector\", \"error\", \"convex\", \"function\", \"estimate\", \"noise\", \"log\", \"use\", \"learn\", \"feature\", \"value\", \"set\", \"constraint\", \"show\", \"method\", \"model\", \"number\", \"result\", \"datum\", \"distribution\", \"figure\", \"give\", \"follow\", \"base\", \"input\", \"also\", \"chess\", \"neurochess\", \"readout\", \"max_product\", \"martinetz\", \"private\", \"regret\", \"board\", \"fritzke\", \"temper\", \"privacy\", \"rhythm\", \"device\", \"moving_target\", \"weighted_matching\", \"ucb\", \"drain\", \"pcp\", \"lateral_connection\", \"ft\", \"rohwer\", \"patient\", \"arrhythmia\", \"nsr\", \"etc_strategy\", \"schulten\", \"simd\", \"topology\", \"bandit\", \"lp_relaxation\", \"processor\", \"matching\", \"layer\", \"network\", \"unit\", \"delay\", \"signal\", \"weight\", \"activation\", \"input\", \"node\", \"function\", \"algorithm\", \"edge\", \"neural\", \"time\", \"error\", \"set\", \"learning\", \"learn\", \"output\", \"use\", \"figure\", \"training\", \"log\", \"result\", \"problem\", \"show\", \"datum\", \"number\", \"value\", \"optimal\", \"model\", \"method\", \"base\", \"adaptor\", \"wavelet\", \"tuning_curve\", \"lasso\", \"geodesic\", \"adaptor_grammar\", \"conquer\", \"eigenfunction\", \"phonology\", \"median_fullset\", \"pcfg\", \"ievel\", \"inhibition\", \"subdifferential\", \"lakoff\", \"wheeler\", \"lift\", \"voxel\", \"suffix\", \"agglomeration\", \"bond\", \"nonterminal\", \"riemannian\", \"ppga\", \"daubechie\", \"quic\", \"log_det\", \"nmax\", \"vowel\", \"kpca\", \"fisher_information\", \"orientation\", \"eigenvector\", \"production\", \"corollary\", \"celi\", \"median\", \"char\", \"decomposition\", \"burst\", \"population\", \"basis\", \"shape_descriptor\", \"selection\", \"gain\", \"cluster\", \"similarity\", \"contrast\", \"density\", \"distribution\", \"word\", \"sample\", \"function\", \"analysis\", \"result\", \"model\", \"datum\", \"method\", \"obtain\", \"probability\", \"cell\", \"use\", \"follow\", \"algorithm\", \"show\", \"give\", \"define\", \"process\", \"let\", \"also\", \"set\", \"problem\", \"matrix\", \"first\", \"parameter\", \"case\", \"base\", \"surrogate\", \"attractiveness\", \"moonsu\", \"qde\", \"rating\", \"nrsfm\", \"calibrate\", \"composite\", \"pred\", \"memorized\", \"distributed_associative\", \"convex_calibrate\", \"quantize\", \"tda\", \"rater\", \"predmap\", \"facial_attractiveness\", \"highlighting\", \"eru\", \"averaged_face\", \"kde\", \"predpd\", \"calibrated_surrogate\", \"human_rater\", \"quantization\", \"non_rigid\", \"subset_ranke\", \"averageness\", \"moonsu_hr\", \"occlusion\", \"participant\", \"video\", \"attractive\", \"dene\", \"low_rank\", \"rotation\", \"rank\", \"trajectory\", \"object\", \"shape\", \"reconstruction\", \"loss\", \"motion\", \"score\", \"density\", \"matrix\", \"human\", \"memory\", \"vector\", \"map\", \"image\", \"condition\", \"mean\", \"result\", \"dimensional\", \"machine\", \"space\", \"use\", \"show\", \"method\", \"figure\", \"give\", \"manifold\", \"point\", \"set\", \"face\", \"eus\", \"urnn\", \"epu\", \"full_capacity\", \"unitary_matrice\", \"urnns\", \"unitary\", \"gplvm\", \"recommendation\", \"degradation\", \"ctw\", \"evoi\", \"restricted_capacity\", \"eus_eus\", \"option\", \"optimal_recommendation\", \"rnl\", \"utility\", \"vgg\", \"alignment\", \"expected_utility\", \"dictionary\", \"visualisation\", \"elicitation\", \"dtw\", \"stft\", \"adamenn\", \"cca\", \"character\", \"word_list\", \"time_warpe\", \"optimise\", \"user\", \"participant\", \"word\", \"human\", \"query\", \"align\", \"response\", \"recognition\", \"set\", \"non\", \"matrix\", \"time\", \"model\", \"use\", \"dimension\", \"visual\", \"choice\", \"show\", \"datum\", \"system\", \"layer\", \"result\", \"space\", \"rate\", \"image\", \"network\", \"parameter\", \"message_estimator\", \"socp_ms\", \"socp_relaxation\", \"soc_constraint\", \"dpf\", \"receptor_afferent\", \"socp\", \"crf\", \"pyramidal_cell\", \"message_passe\", \"eod\", \"fish\", \"cnn_message\", \"electrosensory\", \"deeplab\", \"voc\", \"metzner\", \"qp_rl\", \"feasibility_region\", \"cnns\", \"multiple_source\", \"weakly_electric\", \"deep_structure\", \"voc_extra\", \"marginalization\", \"strictly_dominate\", \"weighted_combining\", \"qp\", \"relaxation\", \"combining_rule\", \"message\", \"dominate\", \"cnn\", \"cycle\", \"potential\", \"adaptation\", \"burst\", \"variable\", \"amplitude\", \"factor\", \"estimation\", \"mixture\", \"inference\", \"constraint\", \"node\", \"define\", \"feature\", \"stimulus\", \"map\", \"distribution\", \"target\", \"learn\", \"vector\", \"spike\", \"set\"], \"Freq\": [518.0, 569.0, 202.0, 320.0, 249.0, 959.0, 166.0, 362.0, 242.0, 144.0, 156.0, 140.0, 148.0, 361.0, 673.0, 983.0, 165.0, 385.0, 230.0, 104.0, 197.0, 153.0, 155.0, 172.0, 51.0, 536.0, 314.0, 143.0, 362.0, 709.0, 155.4949622000105, 80.57866815060555, 58.30871000855674, 66.39299633391948, 51.58041539916062, 45.11842995363195, 46.54929986643324, 56.03946239046185, 41.07586030215392, 55.152906157655664, 98.12062296564433, 44.37476217421779, 71.2492698431532, 95.69829895919578, 24.395403664803652, 24.31319427261948, 65.31953873923126, 26.26199622437751, 211.2963739860113, 23.492811820905725, 62.83964609848844, 22.5683422047273, 21.78946336319562, 20.04874933961933, 21.366057418740443, 20.511754401181825, 22.468626550428418, 30.152851678011903, 71.03568719508688, 16.537832901142377, 31.011625474680358, 427.61299171706264, 270.13882737033555, 73.70485262248629, 52.43367609280671, 311.674335973092, 215.29836837576553, 283.5935488623286, 253.24901804492376, 149.90716481218482, 215.9383221684121, 113.69836156789054, 136.91050699431705, 86.78542578772486, 459.35340565943227, 302.3872335890604, 630.6981621944287, 454.82800657060204, 424.5668910792202, 155.55630305417867, 473.7876689717865, 234.8722334701843, 324.9870514023068, 431.45047908338717, 211.5829156313273, 217.8062401522204, 225.32019330436412, 248.13462322139299, 271.4339414872062, 241.59249097173986, 237.8477722719445, 268.94112917722657, 186.30683649949557, 185.21439280010077, 188.64486278930733, 188.42001495440505, 182.79870142976583, 182.88999944527197, 60.882593147083895, 47.05318742172825, 33.72542269724546, 39.16959451374992, 33.91029842232054, 28.166597596502992, 25.8938443055336, 25.829070259670868, 29.874004664631286, 25.08660735705524, 25.064460662390115, 47.07349744562126, 130.54324023806697, 44.88208615582802, 17.160471229250152, 17.13529106543881, 22.675312212321412, 20.222536746910887, 45.105828204338955, 15.598972842105832, 34.08184821223221, 20.134468975862113, 16.187233606489695, 14.10811992255061, 14.082502512600616, 14.044073435303412, 14.03827429545905, 26.57340064172963, 16.420971883796508, 16.386134386570465, 193.93527547615133, 48.786559089829915, 219.94118750745534, 41.01888262160125, 156.90921557743542, 138.82247809923572, 108.04339288677407, 257.7164858465127, 60.3358150732745, 41.46694023388357, 57.53686627063232, 50.05211767783454, 263.3115300948814, 436.2103226950552, 78.34041189474941, 236.6091904251533, 86.9019140599733, 148.6382533992812, 287.1877382345399, 65.6459976096316, 228.70254789145926, 80.70734255321986, 156.78301378136186, 192.29589395362686, 171.41948345265862, 98.67111077590496, 181.83482392945317, 137.86875539966627, 192.51456916070453, 133.89159296124825, 105.40720392321285, 123.18609168102732, 91.55208954684305, 111.35708822495985, 110.27238601541156, 127.41244413021569, 106.07836845140362, 113.29507686656467, 107.71601520876008, 109.50744937223848, 59.783318217175896, 43.926668243925704, 59.11166217341338, 37.67337402361067, 27.973102721863143, 21.954015260827713, 28.93393330000673, 28.680798979265056, 17.885958315859536, 17.87072278975682, 17.24728475813711, 18.39018290817352, 19.47290983500028, 18.894484892104167, 14.37346088477196, 11.918126867291384, 14.403516146723195, 11.719550601327697, 10.595402467279847, 12.130575959911548, 11.960219560598178, 17.01131316650119, 11.933288795484142, 11.371059186485564, 10.74743526323761, 27.975409295536064, 9.728563248825122, 9.276531714533037, 16.571791000678, 10.120494122852154, 28.602848928981754, 25.87129512675546, 31.949933160584603, 25.602761391781677, 50.26435878094678, 30.7200741481137, 211.40796596351547, 160.23145744878516, 135.20112694331536, 67.34349583581061, 24.959693204219263, 66.8493367061101, 93.25560517671533, 106.7134124601967, 46.17090069340556, 166.3715565661016, 73.17938216539159, 61.523397396027704, 72.14443469821325, 136.29288896201618, 115.27296168207029, 67.90063049671258, 79.33456506054277, 107.421154242079, 46.67020712435511, 88.55915410169602, 82.13742233891593, 109.56623133831147, 73.43747561953003, 83.4794476011287, 89.42362756465333, 80.65692374722853, 73.1369290699484, 70.82428546630892, 64.54180398413536, 63.98039002207703, 61.23238063767366, 59.65147325204558, 34.36269965480667, 27.22490294049362, 22.584505834967324, 22.46897102081049, 20.710579747957865, 20.133936026176332, 61.802458091291825, 21.386549989245495, 18.107238078547436, 17.530334580388466, 17.530541817723275, 16.814802440001944, 27.300238089950778, 14.883175349426278, 14.768044794014108, 14.237127552333147, 12.970545501014188, 12.267917533264248, 12.26861951325047, 19.482943721404766, 11.65282034024149, 12.910563427347213, 10.977368192290886, 10.971089046987913, 10.338090634607514, 10.326778686846135, 10.314830432901816, 22.645232211738197, 29.924720886347576, 21.107840584028878, 15.487005106932104, 32.743519331904054, 88.23836471562646, 230.10029465252933, 75.59536365995163, 26.654863084486855, 57.74122512217536, 115.65752200973003, 18.592322302624716, 130.39350923123396, 62.807039612367724, 192.5676285100763, 175.3312031718203, 50.11270207437392, 80.76428310246575, 109.47881384324843, 90.66410982677573, 117.350715249517, 89.10563836762977, 110.04383038301282, 55.46536806197239, 105.78821463151553, 76.90756711328591, 66.7878177942065, 57.24168974253968, 79.18695596468775, 72.19721427250042, 73.9013122824684, 77.35632058938592, 63.87301008591874, 61.1715284037149, 53.39745469481968, 77.07709309083226, 62.60085889608121, 56.530363644399095, 33.829365000808195, 22.223095133961902, 19.768927343532507, 28.9367990466606, 16.702517670380466, 17.196734734861465, 15.998036298616919, 24.447141295824263, 11.702216160572295, 11.68515726740106, 13.489080573685758, 10.471280827226977, 17.226971632824227, 9.866104386485327, 9.852322324362522, 9.838246414198826, 9.828567753448832, 13.720221835321187, 12.858483986832042, 13.14945113891308, 9.210049040970745, 9.172289240491166, 8.627559108775891, 8.622586597830022, 8.600972732799976, 8.578906642833948, 8.569773138917054, 8.506316359983993, 21.482396500320966, 8.498261389633669, 19.742439135053548, 38.902150772569605, 25.504282201456597, 14.751124532366816, 16.674585745246254, 10.794712625001724, 22.790076564987466, 12.838180330661384, 25.99648879559992, 18.40034154595387, 36.992230129770455, 45.37511656913034, 14.800916395723442, 33.223257932195395, 24.623861114946568, 44.295033265009145, 35.28188793498507, 31.83596795419652, 46.90344051775766, 106.18070816317002, 45.23705380140981, 86.23017749476658, 121.93579037801766, 54.8245051666324, 88.88454807849038, 119.99177027347548, 87.624611448744, 70.95738826576954, 47.00698497574814, 53.740338094637785, 45.855461911841324, 83.06355058732092, 57.619441134799175, 74.17011986538611, 65.63415495797112, 57.560765767417905, 47.408670666497294, 44.644666622171734, 41.96480913843793, 49.934863294994166, 61.53464363584162, 54.37949488722616, 46.671822816536995, 46.24743556489151, 46.872471944717574, 45.17687581687511, 45.70180229220423, 37.02668534834193, 17.81936638706009, 15.89167881367028, 14.364638613418848, 20.87768846510009, 12.186488599596323, 16.55469072516408, 18.68314325955063, 9.996124161457432, 9.567148395626065, 7.825935878752956, 7.814528837603602, 7.798420701047603, 6.931468726214876, 6.9261053951768625, 6.498565553180247, 6.061210757065858, 7.639861921899484, 5.633342389173836, 5.622083087480404, 5.604388047733384, 5.199673627686698, 5.1960284158038474, 5.188275982248356, 12.179816996797314, 7.384271367194103, 4.763332596391564, 4.745610665471645, 5.0490594081847755, 7.375661656006221, 21.909636901074734, 21.844527863657834, 8.683450671848624, 13.082274391576513, 10.81383613078349, 13.524597250886318, 29.86390847255862, 39.02667324196365, 46.33862549750381, 25.387361487394376, 18.780665556020754, 31.878792132025684, 13.940698948690228, 22.4242289775731, 29.92488339712998, 44.28714133177226, 26.439385641892613, 20.63995105727877, 38.74205706801013, 24.391576819117155, 44.647661891106104, 24.659609887123484, 29.633984408419355, 42.65366910909049, 24.998588997015776, 26.69849472837719, 27.778700832072637, 37.39613766495836, 32.234693889421756, 29.428543623238692, 27.315089235548513, 26.830096888640153, 21.005068387228015, 22.75839489150667, 22.62964643547125, 21.034471130876792, 19.566556183669327, 18.807650885184113, 18.387049691091253, 13.691896407690537, 12.51115738493352, 10.16060871150668, 11.725092055753612, 7.666613111551501, 10.159009343987385, 8.605390985922776, 10.698386841882858, 6.6298956189444365, 6.617238648586235, 6.232658392449058, 19.15259052518426, 5.843041968223386, 5.452702149419468, 19.555033811987705, 6.181576022275992, 15.753288523403839, 5.058421805025896, 9.000821619675625, 5.712481420655475, 4.664213962407556, 6.033945635105171, 4.265818871740564, 5.058988833835172, 7.62697141329842, 16.064853540244265, 3.5007000629063727, 4.7953377513785735, 6.230626719568493, 19.390703668902468, 17.91358712804203, 47.08269612021395, 33.0045113362041, 31.071027501624478, 10.348648017208783, 27.372934862953066, 21.735227120172727, 64.209251589445, 27.033812678828244, 29.02243249029762, 36.48905702968437, 45.254357173577304, 38.238150453876536, 15.371223646020788, 14.021668027042546, 13.201540931012287, 19.821089356303485, 20.144455930765982, 16.71327803655265, 14.632034501934447, 17.789572300449905, 15.942511664062712, 15.417104284349978, 15.831668353009725, 15.470634401024155, 15.136916304444226, 6.328889455994439, 5.926017919441861, 4.768711076257675, 4.185551773228802, 3.0409698025281084, 2.66247176658819, 2.6503536014135087, 13.172533820886933, 3.903896513926392, 3.4274012642370586, 2.080069658632716, 3.4273909907360625, 2.0761263465004687, 1.8899613764544632, 1.883987716125403, 2.6499135531208524, 1.6975546838034128, 1.6877088648490026, 1.6875353568321836, 4.433751566788188, 1.772703631338018, 1.5042307958134535, 2.0481070847844056, 1.4979209637018063, 1.4942681633477202, 1.4934658790199502, 1.5841164931822183, 1.6872718605566415, 12.941966018536714, 1.5755642792282045, 11.007987827065456, 4.204185240535048, 4.251743940145177, 5.003545806021511, 8.028867354282747, 4.490206357260664, 3.6248358843754938, 10.983469165688726, 3.481498095980965, 6.616129314338195, 6.410425667398197, 5.829765488611578, 6.856725575660529, 6.062832034704014, 5.944343181718231, 6.639874799639977, 6.040957848583628, 4.558529704883626, 4.3046060492693945, 4.8767677597322105, 4.1268394760374925, 4.708374043908871, 4.299821641805615, 3.890291735107605, 4.133838013215899], \"Total\": [518.0, 569.0, 202.0, 320.0, 249.0, 959.0, 166.0, 362.0, 242.0, 144.0, 156.0, 140.0, 148.0, 361.0, 673.0, 983.0, 165.0, 385.0, 230.0, 104.0, 197.0, 153.0, 155.0, 172.0, 51.0, 536.0, 314.0, 143.0, 362.0, 709.0, 161.82191396027616, 84.23907363924283, 62.05513808797978, 70.69624104724036, 55.18716974952196, 48.90820788337532, 50.46871394884644, 60.781040699785244, 44.63841509065284, 59.94997924254934, 107.70180986778067, 48.954247401124974, 79.02572924686977, 106.18511192274339, 27.897078545077576, 27.856989992970476, 74.87599802588653, 30.147205970998638, 242.83134102615864, 27.00753183320444, 72.31556679323938, 26.120379388904354, 25.285283602515104, 23.528095681349228, 25.189267906346863, 24.21352085868488, 26.73667176927392, 36.35072757201365, 85.70687217128234, 20.00618971031447, 37.51923145701191, 569.626097889155, 362.30971608666397, 93.93527024965944, 65.77632040658666, 449.02267982976673, 319.66710165505583, 440.5382967879811, 392.22781801941744, 221.686076785203, 336.24985634855324, 162.73839127941807, 202.24929208086624, 120.70136551642686, 931.228915653602, 556.836054986392, 1438.405607147639, 959.5196998663876, 887.3729444675466, 255.8795560114438, 1163.9584269804172, 463.1591367533224, 749.0372090681842, 1140.5696247991016, 411.21454300857914, 467.585190573671, 519.9473103678379, 615.4060583496635, 725.7178912015647, 676.6876807512746, 709.5058861716243, 983.6297661376568, 500.1377936622207, 514.6026191435205, 545.6207004805691, 568.2197631348508, 502.88718322612965, 673.2177287761182, 65.00931570334875, 50.89439961489799, 37.415868604312514, 43.56257944858953, 38.2562690890677, 31.837296051081665, 29.495555204131268, 29.471940436945943, 34.1800093843489, 28.718536172621818, 28.718818131986584, 54.78852955893194, 156.25606029295443, 54.01957430831135, 20.763733801155148, 20.775631726704546, 27.54201327692671, 24.639623393436022, 55.42927335865456, 19.204430228092434, 41.98987367958133, 24.90191968850256, 20.031068953834456, 17.635638000904653, 17.63659579879341, 17.615148840422663, 17.612674501619164, 33.3484436314789, 20.648763700710003, 20.606254275254393, 249.10903684287865, 65.0811827702628, 320.00864709422734, 54.48203293118679, 230.65404458273605, 221.09573771192376, 169.73013534544526, 514.6026191435205, 98.04114263048743, 62.475569839098, 94.30265406062553, 79.26491147805761, 673.2177287761182, 1438.405607147639, 144.94128077122645, 709.5058861716243, 174.20668133843645, 394.79087319336827, 1163.9584269804172, 121.89688865103088, 983.6297661376568, 176.72535963981747, 568.2197631348508, 887.3729444675466, 725.7178912015647, 268.5270063816834, 931.228915653602, 545.6207004805691, 1140.5696247991016, 536.5765385014809, 345.2300263814131, 502.88718322612965, 264.67966615101767, 432.5044919565476, 460.1720195180717, 959.5196998663876, 519.9473103678379, 676.6876807512746, 615.4060583496635, 749.0372090681842, 63.87494036961776, 47.83116602416833, 64.82748453295657, 41.91050321596211, 31.71959998221222, 25.66863563631688, 34.05819438824915, 34.28635740418654, 21.61335892546839, 21.610702004460006, 20.971564389035855, 22.393941494339238, 24.262719408027504, 23.965654770505665, 18.337004344133973, 15.60383464945179, 18.92001727441577, 15.65018634160308, 14.25577398143565, 16.32313403637965, 16.138475582934326, 23.082007904248183, 16.242102989748396, 15.48917084279579, 14.818076739127655, 38.73361430720455, 13.546568515781386, 12.919244984529502, 23.27119385071182, 14.318114433666164, 40.72864325713037, 38.42138867759651, 48.41857383482686, 42.35976253457602, 102.71770237837848, 57.20620778770466, 983.6297661376568, 676.6876807512746, 536.5765385014809, 200.68219112658338, 47.068513046914894, 221.31393460204828, 385.8795871819984, 500.1377936622207, 128.42224609315437, 1140.5696247991016, 282.6928448089167, 210.8644318013724, 305.4058457722183, 1163.9584269804172, 887.3729444675466, 314.4618155940949, 460.1720195180717, 959.5196998663876, 150.28073977244117, 725.7178912015647, 615.4060583496635, 1438.405607147639, 502.88718322612965, 749.0372090681842, 931.228915653602, 709.5058861716243, 568.2197631348508, 545.6207004805691, 467.585190573671, 519.9473103678379, 424.73058069570044, 429.137299649584, 39.21432511565205, 31.10423090245739, 26.502001799585535, 26.532853347762135, 24.51723617476585, 23.86024045304456, 73.5529779143744, 25.74079177922285, 21.89053611317192, 21.231291729009445, 21.236117596595278, 20.55175533520884, 33.42903922645976, 18.61367536262453, 18.63567591186462, 17.967228508054088, 16.645480034825724, 15.98010180642351, 15.991932334690475, 25.640297620707358, 15.336062564581404, 17.24225689420946, 14.666700001118876, 14.660121054006977, 14.03386102451001, 14.024766334117006, 14.01588086330207, 31.083548528682673, 41.72793970923989, 29.584403959138946, 21.71734700990436, 50.416057989364006, 165.2374960749903, 518.7781118199363, 154.63050675968512, 43.73545960761056, 120.57158831042732, 320.4331942864857, 27.960866766943948, 424.73058069570044, 172.28137905462853, 1140.5696247991016, 983.6297661376568, 144.03673518578756, 365.847283350131, 673.2177287761182, 500.1377936622207, 959.5196998663876, 556.836054986392, 887.3729444675466, 245.9407919329632, 1163.9584269804172, 568.2197631348508, 440.5382967879811, 305.4058457722183, 749.0372090681842, 676.6876807512746, 725.7178912015647, 931.228915653602, 502.88718322612965, 460.1720195180717, 307.0987646484982, 1438.405607147639, 615.4060583496635, 519.9473103678379, 38.04201292950045, 26.094852326678865, 23.560826650703977, 35.1940862617377, 20.434097273012487, 21.11133741755092, 20.59130982172055, 32.164737100467505, 15.450401522864492, 15.431661385640254, 17.9620522704325, 14.196765108984042, 23.393284151661806, 13.562746878040965, 13.569496412697704, 13.577479912951702, 13.583482858663903, 18.970809642046625, 17.84543586637563, 18.32203545919412, 12.909552518491713, 12.969257110043193, 12.312829082414527, 12.3116707506056, 12.327843780505216, 12.310140650187058, 12.31453147111533, 12.351656087487957, 31.233985726414076, 12.356086424306312, 29.050563948061306, 58.251257873736904, 39.395730838953824, 22.14001427916792, 25.310207829879403, 15.780595661773312, 37.04172079260327, 19.378108921217052, 46.53864105607413, 31.421669676422866, 80.74031321142633, 112.01361750293049, 23.791763655240928, 73.2980779675493, 48.82644540824458, 138.9175347500734, 101.34076780715228, 85.45387018805957, 167.70120284665927, 709.5058861716243, 166.16357591539645, 536.5765385014809, 1140.5696247991016, 260.4915842116085, 749.0372090681842, 1438.405607147639, 931.228915653602, 615.4060583496635, 244.6837427498785, 345.2300263814131, 249.10903684287865, 1163.9584269804172, 467.585190573671, 983.6297661376568, 725.7178912015647, 545.6207004805691, 309.87552923863495, 259.58434671644363, 224.09077568663153, 429.137299649584, 959.5196998663876, 676.6876807512746, 361.2783454542623, 351.5604998409074, 432.5044919565476, 385.204074245368, 519.9473103678379, 45.036809581071324, 21.889762921650522, 20.30263800778853, 18.365929396789845, 27.119245907292104, 16.128428761710225, 22.6265326462529, 25.90032648676396, 13.938674995086174, 13.474964159212783, 11.703711435650524, 11.721117074362155, 11.737807923192076, 10.832481012483505, 10.831389130630516, 10.398232394619383, 9.939070045222923, 12.66001560996678, 9.506205092025926, 9.500038348945663, 9.533751628929089, 9.059235653364379, 9.062754128528699, 9.057927813795757, 21.36709402207118, 13.01086368348918, 8.618601148843663, 8.618872313110366, 9.192213793841955, 13.722212717967615, 45.51628326914446, 46.14595087170312, 17.022053064944267, 28.91333964963206, 22.79174260542844, 31.073736930165058, 93.99246579413565, 144.94128077122645, 202.24929208086624, 95.55896839628817, 59.584595713132394, 153.70698964219628, 41.136221693106265, 97.84739884072575, 167.70120284665927, 361.2783454542623, 148.14131145596966, 93.65841732092434, 385.8795871819984, 143.91451817516614, 569.626097889155, 149.74431908331516, 266.31831812633106, 749.0372090681842, 173.8251322935014, 244.60898865312697, 327.7490173235782, 1163.9584269804172, 725.7178912015647, 615.4060583496635, 568.2197631348508, 545.6207004805691, 146.8088519971352, 463.1591367533224, 959.5196998663876, 242.83134102615864, 23.696091766646884, 22.894382186079564, 22.50650183648116, 17.730755490074948, 16.541439359559305, 14.147751810047692, 16.993459084070395, 11.860778120813482, 15.740290781846046, 13.427209088363323, 17.00509630784487, 10.56160884101477, 10.575931015329077, 10.165619112149216, 31.32315362442229, 9.765335385700007, 9.36497793178117, 33.94427355260711, 10.75808825954967, 27.712840681258314, 8.970368274144732, 16.162214040048994, 10.499425129609119, 8.574219301993391, 11.286126579472121, 8.186541733478904, 9.851652404144495, 14.881855185690442, 32.16668743977885, 7.374490070006773, 10.171038449056946, 13.402481861155668, 45.90162638863485, 45.51628326914446, 166.16357591539645, 148.14131145596966, 140.9795574511126, 28.00353290619888, 155.38218043460472, 120.4047666480462, 959.5196998663876, 242.4936842641476, 361.2783454542623, 673.2177287761182, 1438.405607147639, 1163.9584269804172, 141.02751916456432, 110.18976225148356, 90.18771096979242, 725.7178912015647, 931.228915653602, 394.79087319336827, 165.2374960749903, 749.0372090681842, 327.7490173235782, 262.5391591725181, 569.626097889155, 518.7781118199363, 432.5044919565476, 10.679726972784945, 10.3146833060976, 9.112980810260739, 8.527234475197425, 7.297381430571372, 6.8481564953450444, 6.8983724620521345, 36.18950595296276, 10.805263421710919, 10.192454010040658, 6.2564636478348135, 10.389273498749471, 6.29493431117731, 6.051008654080306, 6.091680778512283, 8.934276932405217, 5.849876997564349, 5.890175291156484, 5.890492198018519, 15.573760915431539, 6.47727436588325, 5.651487467366213, 7.769870240058152, 5.691500847667834, 5.689441782772352, 5.690711280875061, 6.262590284457621, 6.677514534124452, 51.58014196438461, 6.2910335844827125, 44.290890947842186, 19.28105512201411, 21.29053590170998, 37.18604321484533, 104.96852185310789, 44.0091533388226, 31.421669676422866, 362.2912574278757, 30.403611099076166, 144.87383335206306, 143.59165360595392, 117.23354983378113, 197.43867472247211, 150.28073977244117, 172.28137905462853, 309.87552923863495, 314.4618155940949, 138.4583915869021, 143.91451817516614, 709.5058861716243, 162.26970003916992, 887.3729444675466, 385.8795871819984, 156.25606029295443, 959.5196998663876], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.006899833679199, -6.664299964904785, -6.98769998550415, -6.857900142669678, -7.110300064086914, -7.244200229644775, -7.2129998207092285, -7.027400016784668, -7.338099956512451, -7.043399810791016, -6.467299938201904, -7.260799884796143, -6.787300109863281, -6.492300033569336, -7.859099864959717, -7.862500190734863, -6.874199867248535, -7.785399913787842, -5.700200080871582, -7.8968000411987305, -6.912899971008301, -7.9369001388549805, -7.972099781036377, -8.055299758911133, -7.991700172424316, -8.032500267028809, -7.941400051116943, -7.647200107574463, -6.790299892425537, -8.24779987335205, -7.619100093841553, -4.995299816131592, -5.454599857330322, -6.753399848937988, -7.093900203704834, -5.311500072479248, -5.68149995803833, -5.405900001525879, -5.519100189208984, -6.043499946594238, -5.678500175476074, -6.319900035858154, -6.134200096130371, -6.590099811553955, -4.923699855804443, -5.341800212860107, -4.6066999435424805, -4.933599948883057, -5.002399921417236, -6.006499767303467, -4.8927001953125, -5.594399929046631, -5.269700050354004, -4.986299991607666, -5.69890022277832, -5.669899940490723, -5.636000156402588, -5.5395002365112305, -5.44980001449585, -5.566199779510498, -5.581900119781494, -5.459000110626221, -5.826099872589111, -5.831999778747559, -5.813600063323975, -5.814799785614014, -5.845099925994873, -5.844600200653076, -6.50540018081665, -6.7631001472473145, -7.096099853515625, -6.946400165557861, -7.09060001373291, -7.276199817657471, -7.360300064086914, -7.362800121307373, -7.217400074005127, -7.392000198364258, -7.392899990081787, -6.762599945068359, -5.742599964141846, -6.810299873352051, -7.771699905395508, -7.773200035095215, -7.493100166320801, -7.607500076293945, -6.805300235748291, -7.867099761962891, -7.085599899291992, -7.6118998527526855, -7.830100059509277, -7.967599868774414, -7.969399929046631, -7.972099781036377, -7.972599983215332, -7.334400177001953, -7.815800189971924, -7.81790018081665, -5.346799850463867, -6.726900100708008, -5.2210001945495605, -6.900300025939941, -5.558700084686279, -5.681099891662598, -5.93179988861084, -5.0625, -6.514400005340576, -6.889400005340576, -6.5619001388549805, -6.701300144195557, -5.040999889373779, -4.536200046539307, -6.253300189971924, -5.147900104522705, -6.149600028991699, -5.612800121307373, -4.95419979095459, -6.430099964141846, -5.1819000244140625, -6.223499774932861, -5.559500217437744, -5.355299949645996, -5.470200061798096, -6.022500038146973, -5.411200046539307, -5.688000202178955, -5.3541998863220215, -5.717299938201904, -5.956500053405762, -5.800600051879883, -6.097400188446045, -5.901599884033203, -5.911399841308594, -5.766900062561035, -5.950200080871582, -5.884300231933594, -5.934800148010254, -5.918300151824951, -6.0777997970581055, -6.386000156402588, -6.089099884033203, -6.539599895477295, -6.837299823760986, -7.079599857330322, -6.803500175476074, -6.812300205230713, -7.2845001220703125, -7.285399913787842, -7.320899963378906, -7.256700038909912, -7.19950008392334, -7.229700088500977, -7.503200054168701, -7.690499782562256, -7.501100063323975, -7.707300186157227, -7.80810022354126, -7.672800064086914, -7.686999797821045, -7.334700107574463, -7.689199924468994, -7.737500190734863, -7.793900012969971, -6.837200164794922, -7.893499851226807, -7.941100120544434, -7.360799789428711, -7.854000091552734, -6.815000057220459, -6.91540002822876, -6.704400062561035, -6.92579984664917, -6.251299858093262, -6.743599891662598, -4.814799785614014, -5.091899871826172, -5.2617998123168945, -5.958700180053711, -6.951300144195557, -5.966100215911865, -5.633200168609619, -5.4984002113342285, -6.33620023727417, -5.054299831390381, -5.8755998611450195, -6.049099922180176, -5.889900207519531, -5.253699779510498, -5.421199798583984, -5.950500011444092, -5.794899940490723, -5.491799831390381, -6.325399875640869, -5.684899806976318, -5.760200023651123, -5.4720001220703125, -5.872099876403809, -5.743899822235107, -5.67519998550415, -5.778299808502197, -5.876200199127197, -5.908299922943115, -6.001200199127197, -6.010000228881836, -6.053899765014648, -6.079999923706055, -6.592599868774414, -6.825399875640869, -7.01230001449585, -7.017399787902832, -7.098899841308594, -7.127099990844727, -6.0055999755859375, -7.066800117492676, -7.2332000732421875, -7.265600204467773, -7.265600204467773, -7.307300090789795, -6.8225998878479, -7.429299831390381, -7.437099933624268, -7.473700046539307, -7.566800117492676, -7.622499942779541, -7.622499942779541, -7.159999847412109, -7.673999786376953, -7.571499824523926, -7.733699798583984, -7.734300136566162, -7.793700218200684, -7.7947998046875, -7.795899868011475, -7.0096001625061035, -6.730800151824951, -7.079899787902832, -7.389500141143799, -6.6407999992370605, -5.649499893188477, -4.690999984741211, -5.804100036621094, -6.84660005569458, -6.073599815368652, -5.378900051116943, -7.2067999839782715, -5.258999824523926, -5.989500045776367, -4.869100093841553, -4.962900161743164, -6.2153000831604, -5.73799991607666, -5.433800220489502, -5.622399806976318, -5.3643999099731445, -5.639699935913086, -5.428699970245361, -6.113800048828125, -5.468100070953369, -5.786900043487549, -5.927999973297119, -6.082200050354004, -5.757699966430664, -5.850100040435791, -5.8267998695373535, -5.781099796295166, -5.972599983215332, -6.0157999992370605, -6.151800155639648, -5.7846999168396, -5.992800235748291, -6.094799995422363, -6.4994001388549805, -6.91949987411499, -7.036600112915039, -6.655600070953369, -7.205100059509277, -7.176000118255615, -7.248199939727783, -6.82420015335083, -7.5609002113342285, -7.562399864196777, -7.418799877166748, -7.671999931335449, -7.174200057983398, -7.731599807739258, -7.732999801635742, -7.734399795532227, -7.735400199890137, -7.401800155639648, -7.466700077056885, -7.444300174713135, -7.8003997802734375, -7.804500102996826, -7.865699768066406, -7.866300106048584, -7.868800163269043, -7.871399879455566, -7.872399806976318, -7.879899978637695, -6.953400135040283, -7.880799770355225, -7.037899971008301, -6.359600067138672, -6.781799793243408, -7.329400062561035, -7.2067999839782715, -7.641600131988525, -6.894400119781494, -7.468299865722656, -6.762700080871582, -7.10830020904541, -6.409999847412109, -6.205699920654297, -7.326000213623047, -6.517399787902832, -6.816999912261963, -6.229800224304199, -6.457300186157227, -6.560100078582764, -6.172599792480469, -5.355500221252441, -6.208799839019775, -5.563700199127197, -5.217199802398682, -6.016499996185303, -5.533299922943115, -5.23330020904541, -5.547599792480469, -5.758600234985352, -6.170400142669678, -6.036499977111816, -6.195199966430664, -5.601099967956543, -5.966800212860107, -5.714300155639648, -5.836599826812744, -5.967800140380859, -6.161900043487549, -6.22189998626709, -6.28380012512207, -6.110000133514404, -5.901100158691406, -6.024700164794922, -6.177499771118164, -6.186699867248535, -6.173299789428711, -6.210100173950195, -6.198500156402588, -5.705999851226807, -6.437300205230713, -6.551799774169922, -6.6528000831604, -6.278900146484375, -6.817299842834473, -6.510900020599365, -6.389999866485596, -7.015399932861328, -7.059299945831299, -7.260200023651123, -7.261600017547607, -7.263700008392334, -7.381499767303467, -7.382299900054932, -7.446000099182129, -7.515699863433838, -7.284200191497803, -7.588900089263916, -7.59089994430542, -7.593999862670898, -7.669000148773193, -7.6697001457214355, -7.671199798583984, -6.817800045013428, -7.31820011138916, -7.756700038909912, -7.76039981842041, -7.698400020599365, -7.319399833679199, -6.2307000160217285, -6.23360013961792, -7.156199932098389, -6.746300220489502, -6.936800003051758, -6.713099956512451, -5.920899868011475, -5.65339994430542, -5.481599807739258, -6.0833001136779785, -6.384799957275391, -5.8557000160217285, -6.682799816131592, -6.207499980926514, -5.918900012969971, -5.526899814605713, -6.042699813842773, -6.29040002822876, -5.660699844360352, -6.1234002113342285, -5.518799781799316, -6.112400054931641, -5.928699970245361, -5.564499855041504, -6.098800182342529, -6.0329999923706055, -5.993299961090088, -5.696000099182129, -5.844600200653076, -5.9355998039245605, -6.010200023651123, -6.02810001373291, -6.272799968719482, -6.192699909210205, -6.198299884796143, -6.271399974822998, -6.163400173187256, -6.203000068664551, -6.225599765777588, -6.520500183105469, -6.610599994659424, -6.818699836730957, -6.67549991607666, -7.100399971008301, -6.818900108337402, -6.984899997711182, -6.767199993133545, -7.245699882507324, -7.247600078582764, -7.307499885559082, -6.184800148010254, -7.372000217437744, -7.441199779510498, -6.164000034332275, -7.315700054168701, -6.380199909210205, -7.516200065612793, -6.939899921417236, -7.394599914550781, -7.597300052642822, -7.339900016784668, -7.686600208282471, -7.51609992980957, -7.105599880218506, -6.360599994659424, -7.884300231933594, -7.5696001052856445, -7.307799816131592, -6.172500133514404, -6.251699924468994, -5.285399913787842, -5.640600204467773, -5.701000213623047, -6.8003997802734375, -5.827700138092041, -6.058300018310547, -4.975100040435791, -5.840199947357178, -5.769199848175049, -5.540299892425537, -5.324999809265137, -5.4934000968933105, -6.404799938201904, -6.496699810028076, -6.5569000244140625, -6.1504998207092285, -6.134300231933594, -6.321100234985352, -6.454100131988525, -6.258699893951416, -6.368299961090088, -6.401800155639648, -6.37529993057251, -6.3983001708984375, -6.420100212097168, -6.247000217437744, -6.312699794769287, -6.53000020980835, -6.660399913787842, -6.979899883270264, -7.112800121307373, -7.117400169372559, -5.513899803161621, -6.730100154876709, -6.860300064086914, -7.3597002029418945, -6.860300064086914, -7.361599922180176, -7.45550012588501, -7.458700180053711, -7.117599964141846, -7.562900066375732, -7.568699836730957, -7.56879997253418, -6.602799892425537, -7.519599914550781, -7.683800220489502, -7.375199794769287, -7.688000202178955, -7.690400123596191, -7.690999984741211, -7.6321001052856445, -7.568999767303467, -5.531599998474121, -7.637499809265137, -5.69350004196167, -6.656000137329102, -6.644700050354004, -6.481900215148926, -6.008999824523926, -6.590199947357178, -6.804299831390381, -5.695700168609619, -6.844600200653076, -6.202600002288818, -6.2342000007629395, -6.329100131988525, -6.166800022125244, -6.289899826049805, -6.309599876403809, -6.198999881744385, -6.293499946594238, -6.575099945068359, -6.632400035858154, -6.507599830627441, -6.674600124359131, -6.542699813842773, -6.633500099182129, -6.73360013961792, -6.672900199890137], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1187, 1.1142, 1.0963, 1.0958, 1.091, 1.0779, 1.0777, 1.0774, 1.0754, 1.0752, 1.0654, 1.0604, 1.055, 1.0546, 1.0245, 1.0225, 1.022, 1.0206, 1.0195, 1.0192, 1.0181, 1.0124, 1.0098, 0.9986, 0.994, 0.9927, 0.9847, 0.9717, 0.9708, 0.9682, 0.9681, 0.8718, 0.865, 0.916, 0.9319, 0.7935, 0.7633, 0.7181, 0.7211, 0.7673, 0.7157, 0.8, 0.7684, 0.8287, 0.4519, 0.548, 0.3341, 0.4121, 0.4214, 0.6609, 0.2598, 0.4796, 0.3236, 0.1865, 0.4941, 0.3946, 0.3224, 0.2503, 0.1751, 0.1286, 0.0656, -0.1382, 0.1711, 0.1367, 0.0965, 0.0548, 0.1466, -0.1446, 1.5322, 1.5193, 1.4939, 1.4914, 1.4771, 1.4752, 1.4675, 1.4658, 1.4631, 1.4625, 1.4616, 1.446, 1.4179, 1.4124, 1.4071, 1.4051, 1.4033, 1.4002, 1.3916, 1.3898, 1.3891, 1.3852, 1.3847, 1.3746, 1.3727, 1.3712, 1.3709, 1.3706, 1.3686, 1.3686, 1.3474, 1.3096, 1.2227, 1.3139, 1.2125, 1.1323, 1.1461, 0.9062, 1.1123, 1.1879, 1.1037, 1.138, 0.659, 0.4046, 0.9825, 0.4996, 0.9023, 0.6209, 0.1983, 0.9788, 0.1389, 0.814, 0.3101, 0.0685, 0.1547, 0.5966, -0.0357, 0.2221, -0.1814, 0.2096, 0.4114, 0.1911, 0.5361, 0.2409, 0.1691, -0.4213, 0.0082, -0.1895, -0.145, -0.3251, 1.9773, 1.9584, 1.9512, 1.9369, 1.9178, 1.8872, 1.8805, 1.865, 1.8542, 1.8535, 1.848, 1.8466, 1.8236, 1.8058, 1.8, 1.7741, 1.7708, 1.7543, 1.7468, 1.7467, 1.7439, 1.7384, 1.7353, 1.7345, 1.7223, 1.7181, 1.7125, 1.7123, 1.704, 1.6966, 1.6901, 1.648, 1.6278, 1.54, 1.3288, 1.4218, 0.5061, 0.6029, 0.6651, 0.9516, 1.4092, 0.8464, 0.6233, 0.4988, 1.0206, 0.1185, 0.6921, 0.8117, 0.6006, -0.1012, 0.0026, 0.5107, 0.2856, -0.1461, 0.8741, -0.06, 0.0296, -0.5312, 0.1196, -0.1507, -0.2996, -0.1308, -0.0066, 0.0018, 0.0633, -0.0516, 0.1067, 0.0703, 1.9505, 1.9493, 1.9226, 1.9163, 1.9138, 1.9127, 1.9085, 1.8972, 1.8928, 1.891, 1.8908, 1.8819, 1.88, 1.8589, 1.8499, 1.8498, 1.8331, 1.8182, 1.8175, 1.8079, 1.8079, 1.7932, 1.7928, 1.7927, 1.7769, 1.7765, 1.7759, 1.7658, 1.7501, 1.7449, 1.7444, 1.6509, 1.4552, 1.2696, 1.3669, 1.5874, 1.3463, 1.0635, 1.6745, 0.9016, 1.0735, 0.3037, 0.358, 1.0267, 0.5719, 0.2662, 0.3748, -0.0187, 0.2501, -0.0048, 0.5932, -0.3156, 0.0826, 0.1961, 0.4082, -0.1644, -0.1553, -0.2019, -0.4055, 0.0191, 0.0646, 0.3331, -0.8439, -0.203, -0.1364, 2.074, 2.0308, 2.0159, 1.9956, 1.9897, 1.9863, 1.939, 1.917, 1.9135, 1.9133, 1.905, 1.887, 1.8854, 1.8732, 1.8713, 1.8693, 1.8678, 1.8674, 1.8636, 1.8597, 1.8537, 1.845, 1.8357, 1.8352, 1.8314, 1.8303, 1.8289, 1.8184, 1.8171, 1.8171, 1.8051, 1.7877, 1.7566, 1.7853, 1.7741, 1.8117, 1.7057, 1.7797, 1.6091, 1.6563, 1.4109, 1.2877, 1.7167, 1.4001, 1.5068, 1.0484, 1.1363, 1.204, 0.9173, 0.292, 0.8903, 0.3632, -0.0444, 0.633, 0.0599, -0.2925, -0.172, 0.0312, 0.5417, 0.3313, 0.499, -0.4486, 0.0977, -0.3935, -0.2117, -0.0577, 0.314, 0.431, 0.5162, 0.0403, -0.5554, -0.3298, 0.1449, 0.163, -0.0308, 0.0482, -0.2402, 2.6986, 2.6887, 2.6495, 2.6487, 2.6329, 2.6142, 2.582, 2.5678, 2.562, 2.552, 2.492, 2.4891, 2.4856, 2.448, 2.4473, 2.4244, 2.3999, 2.3894, 2.3712, 2.3699, 2.3632, 2.3393, 2.3382, 2.3372, 2.3324, 2.328, 2.3015, 2.2977, 2.2953, 2.2736, 2.1633, 2.1466, 2.2214, 2.1014, 2.1489, 2.0626, 1.7479, 1.5824, 1.421, 1.569, 1.7399, 1.3214, 1.8124, 1.4212, 1.171, 0.7955, 1.1712, 1.3821, 0.5959, 1.1195, 0.3483, 1.0907, 0.6987, 0.0288, 0.9552, 0.6794, 0.4265, -0.5435, -0.2196, -0.1458, -0.1406, -0.1179, 0.9501, -0.1187, -0.8527, 0.4483, 2.8833, 2.8782, 2.8727, 2.8163, 2.7956, 2.7438, 2.7037, 2.6384, 2.6369, 2.6299, 2.6114, 2.6092, 2.6059, 2.5856, 2.5829, 2.5612, 2.5339, 2.5233, 2.5207, 2.51, 2.5019, 2.4894, 2.4661, 2.466, 2.4486, 2.423, 2.4083, 2.4064, 2.3805, 2.3297, 2.3229, 2.3088, 2.2131, 2.1423, 1.8137, 1.5733, 1.5625, 2.0793, 1.3385, 1.3629, 0.3705, 0.8809, 0.5532, 0.1598, -0.3842, -0.3409, 0.8584, 1.0132, 1.1533, -0.5256, -0.7588, -0.0873, 0.6506, -0.6654, 0.0516, 0.2399, -0.5082, -0.4377, -0.2776, 3.5968, 3.5658, 3.4724, 3.4084, 3.2447, 3.1753, 3.1634, 3.1094, 3.1019, 3.0302, 3.0188, 3.011, 3.0108, 2.9563, 2.9465, 2.9046, 2.8828, 2.8701, 2.8699, 2.8637, 2.8242, 2.7964, 2.7867, 2.7851, 2.783, 2.7823, 2.7454, 2.7444, 2.7373, 2.7355, 2.7278, 2.597, 2.5091, 2.1142, 1.5494, 1.8375, 1.9603, 0.6239, 1.9529, 1.0336, 1.0109, 1.1188, 0.7598, 0.9097, 0.7533, 0.2769, 0.1677, 0.7064, 0.6105, -0.8601, 0.4482, -1.1189, -0.377, 0.427, -1.3272]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.43570430501277846, 0.45833829488357214, 0.011316994935396844, 0.07921896454777791, 0.005658497467698422, 0.005658497467698422, 0.005658497467698422, 0.07152854082350735, 0.10729281123526102, 0.10729281123526102, 0.6795211378233198, 0.035764270411753674, 0.035764270411753674, 0.05670336394683896, 0.7938470952557455, 0.05670336394683896, 0.05670336394683896, 0.05670336394683896, 0.04297158136428879, 0.08594316272857758, 0.7305168831929094, 0.04297158136428879, 0.04297158136428879, 0.04297158136428879, 0.9312700150758304, 0.019814255639911287, 0.019814255639911287, 0.019814255639911287, 0.019814255639911287, 0.019814255639911287, 0.20301162870490835, 0.10150581435245418, 0.10150581435245418, 0.10150581435245418, 0.10150581435245418, 0.5075290717622709, 0.06816763723908215, 0.36356073194177146, 0.04544509149272143, 0.2726705489563286, 0.11361272873180359, 0.022722545746360717, 0.022722545746360717, 0.09089018298544287, 0.026286726778974666, 0.026286726778974666, 0.026286726778974666, 0.026286726778974666, 0.8937487104851386, 0.04736791327908243, 0.04736791327908243, 0.04736791327908243, 0.04736791327908243, 0.8052545257444013, 0.029166119579617254, 0.029166119579617254, 0.8458174678089003, 0.029166119579617254, 0.05833223915923451, 0.10915817756462133, 0.054579088782310665, 0.054579088782310665, 0.054579088782310665, 0.7095281541700387, 0.27347688048955815, 0.23281117335356438, 0.21451160514236717, 0.17791246871997277, 0.07523155820158849, 0.0182995682111972, 0.006099856070399067, 0.0010166426783998444, 0.17854890012442667, 0.28567824019908267, 0.03570978002488533, 0.07141956004977067, 0.03570978002488533, 0.35709780024885335, 0.288674845426808, 0.036084355678351, 0.036084355678351, 0.036084355678351, 0.036084355678351, 0.036084355678351, 0.577349690853616, 0.061963721099993645, 0.061963721099993645, 0.7435646531999238, 0.061963721099993645, 0.061963721099993645, 0.061963721099993645, 0.38216207291679305, 0.19807180608492322, 0.13981539253053404, 0.1071918009400761, 0.11651282710877837, 0.03495384813263351, 0.02097230887958011, 0.0023302565421755676, 0.03289082986692938, 0.42758078827008195, 0.06578165973385876, 0.16445414933464692, 0.16445414933464692, 0.09867248960078814, 0.2879171710172191, 0.16123361576964268, 0.10748907717976178, 0.12284465963401346, 0.21113925874596065, 0.05374453858988089, 0.04990564297631797, 0.0038388956135629207, 0.851614288267639, 0.03702670818554952, 0.03702670818554952, 0.03702670818554952, 0.03702670818554952, 0.10488372209999175, 0.08740310174999313, 0.5418992308499574, 0.22724806454998214, 0.017480620349998626, 0.017480620349998626, 0.862434814855207, 0.033170569802123344, 0.033170569802123344, 0.033170569802123344, 0.033170569802123344, 0.033170569802123344, 0.0681816632182913, 0.0681816632182913, 0.0681816632182913, 0.7499982954012044, 0.0681816632182913, 0.8500475461706849, 0.04250237730853425, 0.04250237730853425, 0.04250237730853425, 0.04250237730853425, 0.11749464018055852, 0.11749464018055852, 0.05874732009027926, 0.05874732009027926, 0.05874732009027926, 0.5287258808125134, 0.04568345502778056, 0.04568345502778056, 0.04568345502778056, 0.04568345502778056, 0.04568345502778056, 0.82230219050005, 0.022955481807043866, 0.8952637904747108, 0.022955481807043866, 0.022955481807043866, 0.022955481807043866, 0.1052627329773866, 0.1052627329773866, 0.1052627329773866, 0.1052627329773866, 0.1052627329773866, 0.6315763978643196, 0.11602445931109537, 0.11602445931109537, 0.11602445931109537, 0.11602445931109537, 0.11602445931109537, 0.5801222965554769, 0.023964758551895823, 0.023964758551895823, 0.21568282696706242, 0.7189427565568747, 0.023964758551895823, 0.4327361552093101, 0.2038668108986083, 0.12308939525953709, 0.10962649265302521, 0.08847050284279227, 0.02500253341209347, 0.015386174407442136, 0.001923271800930267, 0.8700713168117948, 0.03954869621871795, 0.03954869621871795, 0.03954869621871795, 0.03954869621871795, 0.08034737383394071, 0.20533217757562625, 0.08034737383394071, 0.15176726168633245, 0.4017368691697035, 0.062492401870842776, 0.017854971963097935, 0.16319679239463034, 0.6119879714798638, 0.040799198098657584, 0.010199799524664396, 0.10199799524664396, 0.010199799524664396, 0.05099899762332198, 0.8497370187005381, 0.04998453051179636, 0.04998453051179636, 0.04998453051179636, 0.04998453051179636, 0.04842904953024453, 0.7748647924839125, 0.04842904953024453, 0.04842904953024453, 0.04842904953024453, 0.9346526619241338, 0.01611470106765748, 0.01611470106765748, 0.01611470106765748, 0.01611470106765748, 0.03884884383421214, 0.03884884383421214, 0.03884884383421214, 0.8158257205184548, 0.03884884383421214, 0.0774620188087538, 0.0774620188087538, 0.0774620188087538, 0.0774620188087538, 0.6971581692787842, 0.9213399335592137, 0.016452498813557388, 0.016452498813557388, 0.016452498813557388, 0.016452498813557388, 0.016452498813557388, 0.03182517066400028, 0.19095102398400166, 0.03182517066400028, 0.03182517066400028, 0.572853071952005, 0.1273006826560011, 0.0441959011411148, 0.0441959011411148, 0.0441959011411148, 0.0441959011411148, 0.0441959011411148, 0.7513303193989517, 0.11034173340884246, 0.11034173340884246, 0.11034173340884246, 0.11034173340884246, 0.11034173340884246, 0.5517086670442123, 0.9615490353904615, 0.011870975745561254, 0.011870975745561254, 0.011870975745561254, 0.011870975745561254, 0.9040815445939943, 0.018835032179041547, 0.009417516089520774, 0.018835032179041547, 0.009417516089520774, 0.04708758044760387, 0.009417516089520774, 0.056700284533844895, 0.7938039834738286, 0.056700284533844895, 0.056700284533844895, 0.056700284533844895, 0.428344379075539, 0.1609536454708086, 0.10124503505421832, 0.12720530045273584, 0.11682119429332882, 0.04413245117747978, 0.01557615923911051, 0.002596026539851752, 0.20158776997673192, 0.0671959233255773, 0.0671959233255773, 0.0671959233255773, 0.0671959233255773, 0.5375673866046184, 0.06456123508154814, 0.06456123508154814, 0.7101735858970294, 0.06456123508154814, 0.06456123508154814, 0.06456123508154814, 0.12673792820411534, 0.06336896410205767, 0.06336896410205767, 0.06336896410205767, 0.6970586051226344, 0.012042919189207094, 0.7787754409020586, 0.0040143063964023645, 0.008028612792804729, 0.18465809423450877, 0.0040143063964023645, 0.008028612792804729, 0.05506402457098347, 0.7525416691367741, 0.09177337428497245, 0.03670934971398898, 0.03670934971398898, 0.01835467485699449, 0.01835467485699449, 0.20641849089930586, 0.051604622724826466, 0.051604622724826466, 0.051604622724826466, 0.670860095422744, 0.09326418847500156, 0.24870450260000418, 0.031088062825000522, 0.031088062825000522, 0.09326418847500156, 0.49740900520000836, 0.05100177024853904, 0.02550088512426952, 0.02550088512426952, 0.8670300942251636, 0.02550088512426952, 0.33263955451811206, 0.17740776240965977, 0.1219678366566411, 0.08870388120482989, 0.1219678366566411, 0.011087985150603736, 0.14414380695784856, 0.6948424077783449, 0.1224882449609262, 0.07349294697655571, 0.051222356983660046, 0.03786000298792264, 0.008908235997158268, 0.011135294996447836, 0.002227058999289567, 0.6423794565910433, 0.1159851796622717, 0.14869894828496372, 0.03568774758839129, 0.03568774758839129, 0.008921936897097823, 0.011895915862797098, 0.0029739789656992744, 0.720787205909114, 0.0662792833019875, 0.1574132978422203, 0.02485473123824531, 0.016569820825496875, 0.008284910412748437, 0.008284910412748437, 0.1655658520098223, 0.2807420968862204, 0.035992576523874414, 0.122374760181173, 0.31673467341009487, 0.07198515304774883, 0.007198515304774883, 0.5636307162675132, 0.046969226355626094, 0.046969226355626094, 0.046969226355626094, 0.09393845271125219, 0.046969226355626094, 0.18787690542250438, 0.15885789280189883, 0.15885789280189883, 0.15885789280189883, 0.15885789280189883, 0.15885789280189883, 0.31771578560379765, 0.5136845263929188, 0.06421056579911485, 0.06421056579911485, 0.06421056579911485, 0.06421056579911485, 0.2568422631964594, 0.05227901328651857, 0.8887432258708157, 0.026139506643259286, 0.026139506643259286, 0.026139506643259286, 0.015655591914660354, 0.015655591914660354, 0.9393355148796212, 0.015655591914660354, 0.015655591914660354, 0.015655591914660354, 0.15895639191413188, 0.15895639191413188, 0.15895639191413188, 0.15895639191413188, 0.15895639191413188, 0.31791278382826377, 0.03860955191090111, 0.03860955191090111, 0.03860955191090111, 0.03860955191090111, 0.15443820764360444, 0.7335814863071211, 0.4140390792755501, 0.14691709264616296, 0.07345854632308148, 0.053424397325877435, 0.14023904298042827, 0.166951241643367, 0.006678049665734679, 0.006678049665734679, 0.04856417627911943, 0.09712835255823886, 0.04856417627911943, 0.04856417627911943, 0.7770268204659109, 0.32605641996570567, 0.0998131897854201, 0.31274799466098296, 0.039925275914168036, 0.11312161509014278, 0.039925275914168036, 0.02661685060944536, 0.039925275914168036, 0.2691510630166143, 0.12872442144272858, 0.08191554091810001, 0.08191554091810001, 0.3744710441970286, 0.035106660393471434, 0.011702220131157143, 0.011702220131157143, 0.2532839419680043, 0.6286869273848679, 0.03618342028114347, 0.03618342028114347, 0.027137565210857605, 0.013568782605428802, 0.004522927535142934, 0.16352307048707987, 0.11680219320505705, 0.3581933924955083, 0.16352307048707987, 0.10122856744438277, 0.07008131592303424, 0.015573625760674274, 0.08531610030475004, 0.08531610030475004, 0.08531610030475004, 0.08531610030475004, 0.08531610030475004, 0.6825288024380003, 0.11852925191939423, 0.03950975063979808, 0.11852925191939423, 0.03950975063979808, 0.6716657608765673, 0.03950975063979808, 0.05207131834284726, 0.8331410934855562, 0.05207131834284726, 0.05207131834284726, 0.05207131834284726, 0.056777294096252574, 0.794882117347536, 0.056777294096252574, 0.056777294096252574, 0.056777294096252574, 0.41448479621402456, 0.1381615987380082, 0.02763231974760164, 0.02763231974760164, 0.02763231974760164, 0.3592201567188213, 0.1764177012403056, 0.0588059004134352, 0.0588059004134352, 0.0588059004134352, 0.0588059004134352, 0.6468649045477872, 0.031526248772392525, 0.031526248772392525, 0.8827349656269907, 0.031526248772392525, 0.031526248772392525, 0.826242937185923, 0.02665299797373945, 0.02665299797373945, 0.02665299797373945, 0.02665299797373945, 0.0533059959474789, 0.02665299797373945, 0.10756723905497774, 0.3495935269286776, 0.026891809763744435, 0.3227017171649332, 0.026891809763744435, 0.026891809763744435, 0.13445904881872217, 0.49289706567782154, 0.19544066656506212, 0.09557263364994796, 0.08268643585444936, 0.09449878383365641, 0.016107747244373253, 0.021476996325831003, 0.0010738498162915502, 0.08111718624966371, 0.08111718624966371, 0.08111718624966371, 0.08111718624966371, 0.7300546762469734, 0.0348207162784755, 0.8705179069618875, 0.0348207162784755, 0.0348207162784755, 0.0348207162784755, 0.17190016335803293, 0.1289251225185247, 0.021487520419754116, 0.04297504083950823, 0.558675530913607, 0.06446256125926235, 0.021487520419754116, 0.2574045560875456, 0.1287022780437728, 0.1287022780437728, 0.1287022780437728, 0.1287022780437728, 0.2574045560875456, 0.1641583064443211, 0.1641583064443211, 0.1641583064443211, 0.1641583064443211, 0.1641583064443211, 0.3283166128886422, 0.37111675220871854, 0.20008033597339608, 0.1129485767591752, 0.10649437237293662, 0.1516738030766067, 0.01936261315871575, 0.012908408772477165, 0.02258971535183504, 0.14895128144934436, 0.07447564072467218, 0.07447564072467218, 0.07447564072467218, 0.07447564072467218, 0.6702807665220497, 0.022864741995897227, 0.27437690395076675, 0.022864741995897227, 0.6173480338892251, 0.022864741995897227, 0.022864741995897227, 0.0345861118818464, 0.0345861118818464, 0.0345861118818464, 0.0345861118818464, 0.4150333425821568, 0.4496194544640032, 0.2027415392547216, 0.08944479673002423, 0.2027415392547216, 0.03577791869200969, 0.28026036308740926, 0.17888959346004846, 0.011925972897336564, 0.86154318919798, 0.03589763288324917, 0.03589763288324917, 0.03589763288324917, 0.03589763288324917, 0.029914111297834726, 0.029914111297834726, 0.029914111297834726, 0.8076810050415376, 0.05982822259566945, 0.029914111297834726, 0.12374542219550616, 0.18561813329325924, 0.06187271109775308, 0.06187271109775308, 0.06187271109775308, 0.5568543998797777, 0.46799376739361714, 0.22690606903932953, 0.035454073287395235, 0.06381733191731143, 0.04963570260235333, 0.042544887944874286, 0.10636221986218572, 0.36243320611213653, 0.21861050527398712, 0.08054071246936367, 0.09779943656994161, 0.08054071246936367, 0.14382270083814944, 0.011505816067051955, 0.005752908033525977, 0.04852895565793723, 0.7764632905269957, 0.04852895565793723, 0.04852895565793723, 0.04852895565793723, 0.08544298152754458, 0.08544298152754458, 0.08544298152754458, 0.08544298152754458, 0.08544298152754458, 0.6835438522203566, 0.33544471531336884, 0.33403528373642194, 0.11416395773270116, 0.03664522100062013, 0.14939974715637436, 0.01973204207725699, 0.004228294730840784, 0.00704715788473464, 0.06195969361332445, 0.22718554324885631, 0.6609033985421274, 0.020653231204441482, 0.020653231204441482, 0.07740390411339648, 0.07740390411339648, 0.6966351370205683, 0.07740390411339648, 0.07740390411339648, 0.3407397088290724, 0.029206260756777635, 0.4867710126129606, 0.009735420252259212, 0.009735420252259212, 0.0876187822703329, 0.038941681009036846, 0.2593219078706569, 0.051864381574131375, 0.10372876314826275, 0.051864381574131375, 0.10372876314826275, 0.051864381574131375, 0.10372876314826275, 0.2074575262965255, 0.13703545710391923, 0.13703545710391923, 0.13703545710391923, 0.13703545710391923, 0.13703545710391923, 0.4111063713117577, 0.06007636895468301, 0.06007636895468301, 0.06007636895468301, 0.7809927964108792, 0.06007636895468301, 0.0154255561079441, 0.0308511122158882, 0.9101078103687019, 0.0154255561079441, 0.0154255561079441, 0.17720871602111207, 0.08860435801055604, 0.08860435801055604, 0.08860435801055604, 0.08860435801055604, 0.5316261480633362, 0.36163940163470565, 0.49940679273364114, 0.005740307962455645, 0.09758523536174596, 0.017220923887366936, 0.005740307962455645, 0.01148061592491129, 0.08345275850595092, 0.04172637925297546, 0.7928012058065338, 0.04172637925297546, 0.04172637925297546, 0.13191079328125993, 0.12496811995066728, 0.24993623990133457, 0.34713366652963135, 0.10414009995888941, 0.03471336665296314, 0.006942673330592627, 0.06217989575829397, 0.031089947879146984, 0.031089947879146984, 0.09326984363744095, 0.7461587490995276, 0.10153384427240701, 0.10153384427240701, 0.025383461068101753, 0.07615038320430526, 0.6599699877706455, 0.050766922136203506, 0.025383461068101753, 0.16526170381952465, 0.16526170381952465, 0.16526170381952465, 0.16526170381952465, 0.16526170381952465, 0.3305234076390493, 0.11662869408617918, 0.11662869408617918, 0.11662869408617918, 0.11662869408617918, 0.11662869408617918, 0.5831434704308959, 0.8284049831862823, 0.07000605491715062, 0.03500302745857531, 0.011667675819525103, 0.03500302745857531, 0.011667675819525103, 0.011667675819525103, 0.15983470156436885, 0.15983470156436885, 0.15983470156436885, 0.15983470156436885, 0.15983470156436885, 0.3196694031287377, 0.04443160502086928, 0.04443160502086928, 0.04443160502086928, 0.04443160502086928, 0.04443160502086928, 0.799768890375647, 0.3718975097603187, 0.11996693863236088, 0.21394104056104357, 0.18194985692574733, 0.05798402033897443, 0.03399063261250225, 0.02199393874926616, 0.1051944482913406, 0.1051944482913406, 0.1051944482913406, 0.1051944482913406, 0.1051944482913406, 0.6311666897480436, 0.38911490693849493, 0.13795892155092093, 0.2582308018773648, 0.06013594016322194, 0.10965965559175767, 0.028299265959163267, 0.010612224734686225, 0.0035374082448954083, 0.3064244954010027, 0.09053451000484171, 0.28553191616911616, 0.020892579231886546, 0.1741048269323879, 0.06964193077295516, 0.006964193077295516, 0.04178515846377309, 0.07125622793709509, 0.07125622793709509, 0.07125622793709509, 0.7125622793709508, 0.07125622793709509, 0.04220105196450735, 0.04220105196450735, 0.04220105196450735, 0.04220105196450735, 0.04220105196450735, 0.844021039290147, 0.09837079168202083, 0.09837079168202083, 0.09837079168202083, 0.09837079168202083, 0.09837079168202083, 0.5902247500921249, 0.09468254458701568, 0.09468254458701568, 0.09468254458701568, 0.09468254458701568, 0.09468254458701568, 0.6627778121091098, 0.5155459688972553, 0.1386137746563375, 0.11915920979229014, 0.10700010675226054, 0.08997736249621908, 0.01945456486404737, 0.009727282432023684, 0.002431820608005921, 0.11147814330904308, 0.11147814330904308, 0.11147814330904308, 0.11147814330904308, 0.11147814330904308, 0.5573907165452154, 0.10755797147556563, 0.7529058003289595, 0.015365424496509376, 0.09219254697905627, 0.015365424496509376, 0.8689158454932321, 0.016472338303189234, 0.012354253727391925, 0.004118084575797308, 0.004118084575797308, 0.08647977609174347, 0.004118084575797308, 0.10061303476582663, 0.10061303476582663, 0.10061303476582663, 0.10061303476582663, 0.10061303476582663, 0.6036782085949598, 0.24849207870765122, 0.3037125406426848, 0.069025577418792, 0.22778440548201362, 0.069025577418792, 0.034512788709396, 0.006902557741879201, 0.048317904193154404, 0.16976510050151433, 0.16976510050151433, 0.16976510050151433, 0.16976510050151433, 0.16976510050151433, 0.33953020100302866, 0.3466239606677602, 0.08268094474643821, 0.2162424708752999, 0.10176116276484702, 0.13992159880166466, 0.06360072672802938, 0.034980399700416165, 0.019080218018408816, 0.3308579042777566, 0.276301547721318, 0.12847142027806505, 0.1355109501563152, 0.058076121495563654, 0.04751682667818845, 0.022878472104312954, 0.07404723290062223, 0.8330313701320002, 0.01851180822515556, 0.03702361645031112, 0.01851180822515556, 0.01851180822515556, 0.3953800272297429, 0.18488994079088697, 0.13084518886739693, 0.10808950384698007, 0.13084518886739693, 0.031289066903073176, 0.014222303137760536, 0.002844460627552107, 0.09625312107919455, 0.09625312107919455, 0.09625312107919455, 0.09625312107919455, 0.09625312107919455, 0.1925062421583891, 0.09625312107919455, 0.2887593632375836, 0.03442273966825127, 0.06884547933650254, 0.1721136983412563, 0.03442273966825127, 0.6884547933650252, 0.4662252021552268, 0.11334832896434412, 0.13901210156004468, 0.0941004995175687, 0.12404156754588602, 0.042772954326167595, 0.01710918173046704, 0.00427729543261676, 0.045681841451031546, 0.045681841451031546, 0.045681841451031546, 0.8222731461185678, 0.045681841451031546, 0.11700332205103463, 0.03900110735034488, 0.03900110735034488, 0.7410210396565526, 0.03900110735034488, 0.056399176028329126, 0.056399176028329126, 0.056399176028329126, 0.056399176028329126, 0.056399176028329126, 0.7895884643966077, 0.3778813591287036, 0.16921369445902504, 0.14554131233263293, 0.16921369445902504, 0.1069640970155495, 0.016658342977831482, 0.011397813616411012, 0.0026302646807102336, 0.015382410800372233, 0.9383270588227063, 0.015382410800372233, 0.015382410800372233, 0.015382410800372233, 0.015382410800372233, 0.048133313737681524, 0.8182663335405859, 0.048133313737681524, 0.048133313737681524, 0.048133313737681524, 0.16384563596860077, 0.061442113488225286, 0.08192281798430039, 0.14336493147252566, 0.5120176124018774, 0.020480704496075097, 0.020480704496075097, 0.36648074032500455, 0.3475899805144373, 0.10956640690129002, 0.052894127469588284, 0.06045043139381518, 0.04911597550747483, 0.011334455886340347, 0.03140970258264207, 0.879471672313978, 0.03140970258264207, 0.03140970258264207, 0.03140970258264207, 0.04992244808825215, 0.7987591694120344, 0.04992244808825215, 0.04992244808825215, 0.04992244808825215, 0.04893781147458419, 0.04893781147458419, 0.04893781147458419, 0.04893781147458419, 0.8319427950679312, 0.34639448216230345, 0.2529229552296184, 0.13012702769060075, 0.08980597685689348, 0.10630095219795556, 0.04948492602318621, 0.02382607549264521, 0.003665550075791571, 0.19442628695744785, 0.6363042118607384, 0.047133645323017656, 0.029458528326886037, 0.04124193965764045, 0.035350233992263246, 0.011783411330754414, 0.023860367289007835, 0.023860367289007835, 0.9066939569822977, 0.023860367289007835, 0.023860367289007835, 0.08431150046093386, 0.08431150046093386, 0.08431150046093386, 0.08431150046093386, 0.08431150046093386, 0.6744920036874709, 0.3569589964683085, 0.14459098591121355, 0.3027373767516034, 0.058740088026430506, 0.04066621478752881, 0.08585089788478305, 0.004518468309725424, 0.009036936619450847, 0.06984159853120753, 0.06984159853120753, 0.6984159853120753, 0.06984159853120753, 0.06984159853120753, 0.041215495393692035, 0.041215495393692035, 0.7830944124801488, 0.041215495393692035, 0.08243099078738407, 0.07898884415377304, 0.15797768830754608, 0.07898884415377304, 0.07898884415377304, 0.07898884415377304, 0.6319107532301843, 0.029986406893545727, 0.8096329861257346, 0.029986406893545727, 0.029986406893545727, 0.11994562757418291, 0.04249125095595281, 0.27619313121369327, 0.5311406369494102, 0.04249125095595281, 0.021245625477976406, 0.08498250191190562, 0.3037640180023304, 0.14175654173442084, 0.006750311511162898, 0.06075280360046608, 0.08100373813395477, 0.17550809929023534, 0.22276027986837563, 0.11040052653951836, 0.11040052653951836, 0.11040052653951836, 0.11040052653951836, 0.11040052653951836, 0.5520026326975918, 0.07043858177009471, 0.07043858177009471, 0.07043858177009471, 0.07043858177009471, 0.7043858177009471, 0.7513700681658123, 0.04037736347620019, 0.035110750848869735, 0.012288762797104407, 0.052666126273304595, 0.0789991894099569, 0.028088600679095785, 0.0017555375424434866, 0.026726627960328604, 0.9087053506511725, 0.026726627960328604, 0.026726627960328604, 0.026726627960328604, 0.38492965021583897, 0.3697350587599506, 0.09116754873533028, 0.015194591455888381, 0.08610268491670083, 0.01012972763725892, 0.00506486381862946, 0.035454046730406225, 0.2197171926764699, 0.3686780012706868, 0.09682452558624098, 0.1154446266605181, 0.1154446266605181, 0.04468824257826507, 0.037240202148554225, 0.0037240202148554223, 0.042747311301690925, 0.1282419339050728, 0.042747311301690925, 0.042747311301690925, 0.7267042921287458, 0.3037219495443453, 0.186000263674444, 0.14362045676127955, 0.3060763832617433, 0.025898770891378278, 0.014126602304388152, 0.016481036021786178, 0.0023544337173980255, 0.046267681180347985, 0.046267681180347985, 0.8328182612462638, 0.046267681180347985, 0.046267681180347985, 0.10489050259769861, 0.10489050259769861, 0.10489050259769861, 0.10489050259769861, 0.10489050259769861, 0.6293430155861917, 0.9422479941626365, 0.018120153733896856, 0.018120153733896856, 0.018120153733896856, 0.018120153733896856, 0.08093177448426121, 0.08093177448426121, 0.08093177448426121, 0.08093177448426121, 0.6474541958740897, 0.7452187672919497, 0.02208055606790962, 0.1159229193565255, 0.016560417050932214, 0.049681251152796646, 0.04140104262733054, 0.0027600695084887025, 0.005520139016977405, 0.07369470241093445, 0.07369470241093445, 0.07369470241093445, 0.07369470241093445, 0.7369470241093445, 0.028413864549942294, 0.05682772909988459, 0.028413864549942294, 0.028413864549942294, 0.8240020719483265, 0.17227675154301328, 0.5414412191351845, 0.18048040637839485, 0.008203654835381585, 0.04101827417690792, 0.008203654835381585, 0.05742558384767109, 0.06253153021606724, 0.06253153021606724, 0.06253153021606724, 0.7503783625928068, 0.06253153021606724, 0.29049096688209314, 0.03631137086026164, 0.0302594757168847, 0.5325667726171707, 0.00605189514337694, 0.00605189514337694, 0.0907784271506541, 0.20653894925865413, 0.025817368657331766, 0.7228863224052895, 0.025817368657331766, 0.025817368657331766, 0.03895805036809846, 0.03895805036809846, 0.8570771080981661, 0.03895805036809846, 0.03895805036809846, 0.47894180530263314, 0.2163690038073072, 0.1295960179054184, 0.12396140843126975, 0.019157672212105327, 0.0225384378965945, 0.003380765684489175, 0.005634609474148625, 0.5423499381831162, 0.15264816140915521, 0.09518061829041444, 0.15983160429899781, 0.012571025057224548, 0.026937910836909745, 0.007183442889842599, 0.0017958607224606497, 0.36592313873137183, 0.0981745006352461, 0.12941184174646078, 0.13387431904806288, 0.187424046667288, 0.07139963682563354, 0.013387431904806287, 0.004462477301602096, 0.07361882150586833, 0.07361882150586833, 0.07361882150586833, 0.07361882150586833, 0.7361882150586834, 0.05285407436452137, 0.05285407436452137, 0.7399570411032992, 0.05285407436452137, 0.05285407436452137, 0.061568381916502726, 0.061568381916502726, 0.7388205829980327, 0.061568381916502726, 0.061568381916502726, 0.672574684372747, 0.20020827813886422, 0.0469238151887963, 0.056308578226555564, 0.00938476303775926, 0.0031282543459197534, 0.00938476303775926, 0.157167914971084, 0.2586721933899091, 0.235751872456626, 0.18663689902816225, 0.11132727310451783, 0.03274331561897583, 0.00982299468569275, 0.0065486631237951665, 0.08120487590986113, 0.08120487590986113, 0.08120487590986113, 0.08120487590986113, 0.7308438831887502, 0.5009531458474523, 0.03903531006603525, 0.09108239015408225, 0.06505885011005874, 0.019517655033017625, 0.20818832035218798, 0.052047080088046996, 0.019517655033017625, 0.08775107874040525, 0.3071287755914184, 0.043875539370202624, 0.043875539370202624, 0.043875539370202624, 0.4826309330722289, 0.06760318723210842, 0.03380159361605421, 0.06760318723210842, 0.7098334659371385, 0.03380159361605421, 0.10140478084816264, 0.8805385120007155, 0.0382842831304659, 0.0382842831304659, 0.0382842831304659, 0.0382842831304659, 0.44152097841813864, 0.08993945856665787, 0.15943813109543897, 0.08585130135908252, 0.057234200906055015, 0.11038024460453466, 0.057234200906055015, 0.5313031124412176, 0.01362315672926199, 0.06811578364630995, 0.034057891823154975, 0.18391261584503688, 0.1430431456572509, 0.02724631345852398, 0.36132560258241625, 0.09033140064560406, 0.06948569280431081, 0.13202281632819055, 0.12507424704775946, 0.16676566273034596, 0.020845707841293245, 0.027794277121724328, 0.8711817219123245, 0.013828281300195627, 0.04148484390058688, 0.04148484390058688, 0.013828281300195627, 0.013828281300195627, 0.013828281300195627, 0.17576416776563267, 0.17576416776563267, 0.17576416776563267, 0.17576416776563267, 0.17576416776563267, 0.17576416776563267, 0.04078763172454329, 0.04078763172454329, 0.04078763172454329, 0.8565402662154091, 0.04078763172454329, 0.1586796016794434, 0.03966990041986085, 0.05950485062979127, 0.654553356927704, 0.05950485062979127, 0.019834950209930424, 0.019834950209930424, 0.2075961677296127, 0.2020602699234897, 0.15223718966838265, 0.0996461605102141, 0.13009359844389062, 0.12178975173470612, 0.08027051818878359, 0.002767948903061503, 0.03768912400385853, 0.03768912400385853, 0.03768912400385853, 0.8291607280848876, 0.03768912400385853, 0.04816089483599366, 0.8187352122118923, 0.04816089483599366, 0.04816089483599366, 0.04816089483599366, 0.04627336954596017, 0.04627336954596017, 0.832920651827283, 0.04627336954596017, 0.04627336954596017, 0.17272563270762092, 0.3266767401209352, 0.16146091753103695, 0.10513734164811708, 0.09011772141267178, 0.11264715176583973, 0.02628433541202927, 0.003754905058861324, 0.1619795158436084, 0.05399317194786946, 0.10798634389573893, 0.02699658597393473, 0.6209214774004989, 0.02699658597393473, 0.06480183662729523, 0.06480183662729523, 0.06480183662729523, 0.06480183662729523, 0.7776220395275427, 0.07421170017111353, 0.07421170017111353, 0.07421170017111353, 0.07421170017111353, 0.07421170017111353, 0.7421170017111353, 0.3203129078852976, 0.2135419385901984, 0.02135419385901984, 0.1708335508721587, 0.01067709692950992, 0.22421903551970832, 0.04270838771803968, 0.06773401789394705, 0.11289002982324509, 0.04515601192929803, 0.09031202385859606, 0.3838261013990333, 0.022578005964649015, 0.24835806561113918, 0.09363535252804601, 0.09363535252804601, 0.09363535252804601, 0.09363535252804601, 0.09363535252804601, 0.5618121151682761, 0.09811179908340945, 0.1962235981668189, 0.09811179908340945, 0.09811179908340945, 0.09811179908340945, 0.09811179908340945, 0.29433539725022834, 0.4029859580275541, 0.17549388494748325, 0.1332453570897558, 0.1023714328860319, 0.1153709799191788, 0.04712335799515754, 0.017874377170576997, 0.006499773516573454, 0.17094376521358642, 0.17094376521358642, 0.17094376521358642, 0.17094376521358642, 0.17094376521358642, 0.34188753042717285, 0.4862089400245672, 0.17059962807879553, 0.025589944211819326, 0.1791296094827353, 0.05970986982757843, 0.008529981403939776, 0.008529981403939776, 0.05117988842363865, 0.4386801586871412, 0.30311339015466493, 0.07647356173626867, 0.05353149321538807, 0.08342570371229309, 0.013209069754446407, 0.03128463889210991, 0.0006952141976024424, 0.07381943248838457, 0.07381943248838457, 0.7381943248838457, 0.07381943248838457, 0.07381943248838457, 0.07381943248838457, 0.04925468304248829, 0.04925468304248829, 0.04925468304248829, 0.04925468304248829, 0.04925468304248829, 0.7880749286798127, 0.10878772213391291, 0.10878772213391291, 0.10878772213391291, 0.10878772213391291, 0.10878772213391291, 0.5439386106695646, 0.24309476146361372, 0.21878528531725236, 0.024309476146361374, 0.07292842843908412, 0.024309476146361374, 0.3403326660490592, 0.0972379045854455, 0.053723941162526, 0.053723941162526, 0.053723941162526, 0.80585911743789, 0.053723941162526, 0.1543859258559659, 0.1543859258559659, 0.1543859258559659, 0.1543859258559659, 0.1543859258559659, 0.3087718517119318, 0.026027169616154436, 0.10410867846461774, 0.6767064100200153, 0.026027169616154436, 0.15616301769692661, 0.8681019514094203, 0.040066243911204016, 0.02671082927413601, 0.040066243911204016, 0.013355414637068005, 0.7905580561297688, 0.030406079081914186, 0.06081215816382837, 0.06081215816382837, 0.015203039540957093, 0.015203039540957093, 0.9099197137012737, 0.018569790075536198, 0.018569790075536198, 0.027854685113304297, 0.009284895037768099, 0.009284895037768099, 0.009284895037768099, 0.32962069159030505, 0.09445271279488274, 0.07710425534276141, 0.4433494682208781, 0.015420851068552283, 0.0038552127671380707, 0.02891409575353553, 0.0038552127671380707, 0.3635396681973267, 0.22687062000284297, 0.06286776216946251, 0.22140385807506363, 0.09293495277224893, 0.013666904819448373, 0.01913366674722772, 0.032149967094058415, 0.032149967094058415, 0.032149967094058415, 0.8680491115395772, 0.032149967094058415, 0.03468397883276825, 0.6806730845930767, 0.004335497354096031, 0.16908439680974519, 0.0997164391442087, 0.004335497354096031, 0.004335497354096031, 0.08096080338676083, 0.08096080338676083, 0.08096080338676083, 0.08096080338676083, 0.7286472304808475, 0.104480240980033, 0.2670050602823066, 0.1335025301411533, 0.36568084343011553, 0.08126240965113678, 0.011608915664448112, 0.005804457832224056, 0.034826746993344336, 0.18021057261944864, 0.24186155798926004, 0.29402777637910044, 0.11855958724963728, 0.08062051932975335, 0.06165098536981138, 0.018969533959941963, 0.36701986804346043, 0.18969566213482222, 0.09072401232534977, 0.11134310603565653, 0.09484783106741111, 0.03299054993649082, 0.11134310603565653, 0.004123818742061353, 0.23057654533780164, 0.07685884844593388, 0.07685884844593388, 0.07685884844593388, 0.07685884844593388, 0.5380119391215372, 0.126159227500913, 0.630796137504565, 0.0252318455001826, 0.126159227500913, 0.0630796137504565, 0.0126159227500913, 0.0252318455001826, 0.07710541872329876, 0.07710541872329876, 0.07710541872329876, 0.07710541872329876, 0.6939487685096889, 0.06200231992679007, 0.06200231992679007, 0.06200231992679007, 0.06200231992679007, 0.06200231992679007, 0.7440278391214808, 0.06821226075255873, 0.06821226075255873, 0.06821226075255873, 0.750334868278146, 0.06821226075255873, 0.8987984155791326, 0.02042723671770756, 0.02042723671770756, 0.02042723671770756, 0.02042723671770756, 0.36389871546539637, 0.24458766121444672, 0.1451617826719887, 0.12726512453434627, 0.06164404469632397, 0.03579331627528489, 0.017896658137642445, 0.0019885175708491605, 0.6773818518248393, 0.03461075155309398, 0.019777572316053704, 0.009888786158026852, 0.009888786158026852, 0.2274420816346176, 0.014833179237040278, 0.3106050248613738, 0.24930140153347108, 0.09808579732464436, 0.08582507265906382, 0.19208468642742854, 0.044955990440461994, 0.008173816443720363, 0.008173816443720363, 0.1457490888026562, 0.1457490888026562, 0.0728745444013281, 0.0728745444013281, 0.0728745444013281, 0.5101218108092966, 0.27678392030423843, 0.23770854332011063, 0.12373869378307129, 0.172582915013231, 0.1204824123677273, 0.013025125661375925, 0.04558793981481574, 0.006512562830687962, 0.10240303691610661, 0.10240303691610661, 0.10240303691610661, 0.10240303691610661, 0.10240303691610661, 0.6144182214966396, 0.22383913898028707, 0.07461304632676236, 0.07461304632676236, 0.07461304632676236, 0.07461304632676236, 0.44767827796057413, 0.12770106254183958, 0.031925265635459894, 0.1596263281772995, 0.031925265635459894, 0.031925265635459894, 0.606580047073738, 0.05150103378887851, 0.20600413515551405, 0.017167011262959504, 0.017167011262959504, 0.6695134392554207, 0.03433402252591901, 0.017167011262959504, 0.33341358038056157, 0.2033009636466839, 0.15857475164441343, 0.22363106001135227, 0.020330096364668388, 0.020330096364668388, 0.028462134910535744, 0.008132038545867356, 0.35837778076897375, 0.25664473332487797, 0.1271663093051197, 0.10404516215873433, 0.10866939158801141, 0.009248458858554161, 0.034681720719578105, 0.0023121147146385403, 0.06591047828445394, 0.021970159428151313, 0.021970159428151313, 0.021970159428151313, 0.021970159428151313, 0.48334350741932885, 0.3954628697067236, 0.11664570959931088, 0.6150410142509118, 0.08483324334495336, 0.10604155418119171, 0.04241662167247668, 0.03181246625435751, 0.0579970479581379, 0.0579970479581379, 0.0579970479581379, 0.7539616234557928, 0.0579970479581379, 0.1113458512361763, 0.05567292561808815, 0.05567292561808815, 0.05567292561808815, 0.7237480330351459, 0.06257782410360056, 0.06257782410360056, 0.06257782410360056, 0.7509338892432067, 0.06257782410360056, 0.019648527295079366, 0.9234807828687303, 0.019648527295079366, 0.019648527295079366, 0.019648527295079366, 0.9174315103175865, 0.01668057291486521, 0.01668057291486521, 0.03336114582973042, 0.01668057291486521, 0.023815265738374346, 0.8097190351047278, 0.023815265738374346, 0.07144579721512304, 0.04763053147674869, 0.06472323703174548, 0.06472323703174548, 0.06472323703174548, 0.06472323703174548, 0.7766788443809458, 0.8336883818171108, 0.039699446753195755, 0.039699446753195755, 0.039699446753195755, 0.039699446753195755, 0.0348203744110979, 0.8705093602774474, 0.0348203744110979, 0.0348203744110979, 0.0348203744110979, 0.03393057888873862, 0.8821950511072041, 0.03393057888873862, 0.03393057888873862, 0.03393057888873862, 0.5073850030192981, 0.18784040537310187, 0.09499974524616646, 0.06909072381539379, 0.08204523453078012, 0.04965895774231428, 0.010795425596155279, 0.27811748466219954, 0.6874814227604933, 0.012499662232008968, 0.012499662232008968, 0.003124915558002242, 0.006249831116004484, 0.003124915558002242, 0.8252929722126645, 0.08252929722126645, 0.02750976574042215, 0.02750976574042215, 0.02750976574042215, 0.02750976574042215, 0.03715616004788351, 0.4334885338919743, 0.03715616004788351, 0.024770773365255672, 0.45825930725722996, 0.012385386682627836, 0.9578430770386834, 0.00617963275508828, 0.00617963275508828, 0.00617963275508828, 0.00617963275508828, 0.01235926551017656, 0.00617963275508828, 0.35248662500723316, 0.22863997297766475, 0.26674663514060887, 0.03810666216294412, 0.01905333108147206, 0.00952666554073603, 0.00952666554073603, 0.07621332432588825, 0.08122374454749051, 0.08122374454749051, 0.08122374454749051, 0.08122374454749051, 0.7310137009274146, 0.07174283067454632, 0.07174283067454632, 0.07174283067454632, 0.07174283067454632, 0.07174283067454632, 0.7174283067454631, 0.09617019143729226, 0.09617019143729226, 0.09617019143729226, 0.09617019143729226, 0.09617019143729226, 0.5770211486237536, 0.11038458853078012, 0.11038458853078012, 0.11038458853078012, 0.11038458853078012, 0.11038458853078012, 0.5519229426539006, 0.0470895866653294, 0.0470895866653294, 0.0470895866653294, 0.8476125599759292, 0.0470895866653294, 0.041910726003283015, 0.041910726003283015, 0.041910726003283015, 0.8382145200656603, 0.041910726003283015, 0.04465493491857076, 0.04465493491857076, 0.8037888285342738, 0.04465493491857076, 0.04465493491857076, 0.058723019112547126, 0.029361509556273563, 0.8514837771319334, 0.029361509556273563, 0.029361509556273563, 0.2954551811994172, 0.30414503946998833, 0.1477275905997086, 0.03765605250580808, 0.15641744887027972, 0.028966194235236983, 0.026069574811713285, 0.0028966194235236983, 0.3576243618493629, 0.16698988797098352, 0.23644585907395896, 0.10640063658328153, 0.07980047743746115, 0.02955573238424487, 0.01625565281133468, 0.00443335985763673, 0.2889234306640457, 0.2966280554817536, 0.1271263094921801, 0.06548931095051702, 0.1733540583984274, 0.019261562044269712, 0.03081849927083154, 0.046046139961015614, 0.046046139961015614, 0.13813841988304684, 0.6906920994152342, 0.046046139961015614, 0.046046139961015614, 0.09033417841477405, 0.045167089207387025, 0.09033417841477405, 0.09033417841477405, 0.6775063381108053, 0.033903413347511284, 0.8814887470352935, 0.033903413347511284, 0.033903413347511284, 0.033903413347511284, 0.9184913917023296, 0.022402229065910478, 0.022402229065910478, 0.022402229065910478, 0.022402229065910478, 0.09254748921629333, 0.27764246764888, 0.09254748921629333, 0.09254748921629333, 0.09254748921629333, 0.09254748921629333, 0.3701899568651733, 0.05444864664321254, 0.05444864664321254, 0.05444864664321254, 0.05444864664321254, 0.05444864664321254, 0.7622810530049756, 0.14975631949427406, 0.14975631949427406, 0.14975631949427406, 0.14975631949427406, 0.14975631949427406, 0.29951263898854813, 0.16977423430868027, 0.16977423430868027, 0.16977423430868027, 0.16977423430868027, 0.16977423430868027, 0.33954846861736054, 0.04680093600781876, 0.04680093600781876, 0.04680093600781876, 0.2340046800390938, 0.04680093600781876, 0.5616112320938251, 0.08519478309268941, 0.08519478309268941, 0.08519478309268941, 0.08519478309268941, 0.08519478309268941, 0.6815582647415153, 0.45396652647454394, 0.014186453952329498, 0.2766358520704252, 0.007093226976164749, 0.014186453952329498, 0.014186453952329498, 0.2198900362611072, 0.08123384032860782, 0.08123384032860782, 0.08123384032860782, 0.08123384032860782, 0.7311045629574704, 0.3510919702040515, 0.10639150612243983, 0.1276698073469278, 0.053195753061219916, 0.03191745183673195, 0.31917451836731947, 0.010639150612243983, 0.3123343590283866, 0.27043584745140786, 0.10284180114349314, 0.1714030019058219, 0.0647522451644216, 0.019044777989535765, 0.0571343339686073, 0.09232426126876564, 0.09232426126876564, 0.09232426126876564, 0.09232426126876564, 0.09232426126876564, 0.6462698288813594, 0.0737483633150072, 0.0368741816575036, 0.0368741816575036, 0.0368741816575036, 0.0368741816575036, 0.7743578148075756, 0.0368741816575036, 0.06748514112897423, 0.06748514112897423, 0.7423365524187165, 0.06748514112897423, 0.06748514112897423, 0.06748514112897423, 0.14405630910096456, 0.6562565192377274, 0.06402502626709536, 0.0800312828338692, 0.01600625656677384, 0.01600625656677384, 0.037732998720709426, 0.037732998720709426, 0.037732998720709426, 0.8678589705763169, 0.037732998720709426, 0.1460247003233262, 0.1460247003233262, 0.1460247003233262, 0.1460247003233262, 0.1460247003233262, 0.4380741009699786, 0.415265951606006, 0.22424361386724323, 0.0415265951606006, 0.04983191419272072, 0.01661063806424024, 0.06644255225696095, 0.18271701870664264, 0.06353122784449083, 0.06353122784449083, 0.06353122784449083, 0.06353122784449083, 0.06353122784449083, 0.6353122784449085, 0.1678286120819648, 0.03356572241639296, 0.20139433449835778, 0.03356572241639296, 0.2517429181229472, 0.3188743629557331, 0.04015754658712955, 0.8031509317425909, 0.04015754658712955, 0.04015754658712955, 0.04015754658712955, 0.04015754658712955, 0.013595642601501942, 0.013595642601501942, 0.08157385560901165, 0.8429298412931204, 0.013595642601501942, 0.027191285203003884, 0.061262745118142316, 0.061262745118142316, 0.7351529414177078, 0.061262745118142316, 0.061262745118142316, 0.05816191824503817, 0.03877461216335878, 0.44590803987862593, 0.07754922432671756, 0.13571114257175573, 0.01938730608167939, 0.25203497906183203, 0.018251995591967374, 0.8578437928224666, 0.018251995591967374, 0.0730079823678695, 0.018251995591967374, 0.04505021090849006, 0.45693785350039917, 0.01930723324649574, 0.057921699739487215, 0.18663658804945882, 0.051485955323988634, 0.17376509921846164, 0.006435744415498579, 0.09455432326010538, 0.09455432326010538, 0.09455432326010538, 0.09455432326010538, 0.09455432326010538, 0.6618802628207376, 0.4338903275637079, 0.14685518779079343, 0.11080891442396232, 0.10546872577702437, 0.11881919739436923, 0.05740702795458289, 0.024030848911220745, 0.0040051414852034575, 0.04865764425906827, 0.04865764425906827, 0.04865764425906827, 0.8271799524041605, 0.04865764425906827, 0.056769318786863324, 0.7947704630160866, 0.056769318786863324, 0.056769318786863324, 0.056769318786863324, 0.08121610340780443, 0.08121610340780443, 0.08121610340780443, 0.08121610340780443, 0.7309449306702399, 0.02925686733304123, 0.8777060199912369, 0.02925686733304123, 0.02925686733304123, 0.02925686733304123, 0.10678081756139336, 0.10678081756139336, 0.10678081756139336, 0.10678081756139336, 0.10678081756139336, 0.5339040878069669, 0.06520578510872128, 0.06520578510872128, 0.06520578510872128, 0.7824694213046554, 0.06520578510872128, 0.8603051377304448, 0.0358460474054352, 0.0358460474054352, 0.0358460474054352, 0.0358460474054352, 0.257452137732232, 0.032181517216529, 0.032181517216529, 0.064363034433058, 0.19308910329917403, 0.4505412410314061, 0.032181517216529, 0.08664757452196804, 0.04332378726098402, 0.7365043834367284, 0.04332378726098402, 0.04332378726098402, 0.04332378726098402, 0.2497313810518575, 0.2497313810518575, 0.25159504807463257, 0.03354600640995101, 0.1602753639586548, 0.03913700747827618, 0.014909336182200448, 0.03630816636188861, 0.8350878263234381, 0.03630816636188861, 0.03630816636188861, 0.07261633272377722, 0.07130243571811777, 0.07130243571811777, 0.07130243571811777, 0.7130243571811777, 0.07130243571811777, 0.36791984688934, 0.030659987240778334, 0.051099978734630555, 0.01021999574692611, 0.20439991493852222, 0.22483990643237445, 0.10219995746926111, 0.7005107959084137, 0.0307241577152813, 0.036868989258337564, 0.0061448315430562605, 0.13518629394723772, 0.049158652344450084, 0.04301382080139382, 0.0061448315430562605, 0.12278630285482385, 0.06821461269712437, 0.13642922539424873, 0.04092876761827462, 0.45021644380102077, 0.12278630285482385, 0.054571690157699485, 0.47419557937513784, 0.13235788699042308, 0.1115141252596478, 0.12193600612503544, 0.06461566136540339, 0.023970325990391582, 0.06670003753848093, 0.004168752346155058, 0.1674358803628681, 0.19883010793090589, 0.03139422756803777, 0.06278845513607555, 0.26161856306698145, 0.26161856306698145, 0.010464742522679257, 0.2521881137919887, 0.04203135229866478, 0.04203135229866478, 0.04203135229866478, 0.6304702844799718, 0.37342334161185925, 0.23562875061117317, 0.1226371859906106, 0.10196799734050768, 0.0909444300604528, 0.04409426912021954, 0.027558918200137214, 0.004133837730020582, 0.10781976236831019, 0.1990518689876496, 0.0497629672469124, 0.4810420167201532, 0.08293827874485399, 0.0248814836234562, 0.0497629672469124, 0.0165876557489708, 0.07134763842194967, 0.07134763842194967, 0.07134763842194967, 0.7134763842194968, 0.07134763842194967, 0.41444327795033525, 0.03947078837622241, 0.15788315350488963, 0.009867697094055602, 0.3453693982919461, 0.029603091282166805, 0.05453453471640263, 0.05453453471640263, 0.7634834860296368, 0.05453453471640263, 0.05453453471640263, 0.9200909611594298, 0.020446465803542885, 0.020446465803542885, 0.020446465803542885, 0.020446465803542885, 0.11727131497423117, 0.11727131497423117, 0.11727131497423117, 0.11727131497423117, 0.11727131497423117, 0.4690852598969247, 0.14496172908914215, 0.14496172908914215, 0.14496172908914215, 0.14496172908914215, 0.14496172908914215, 0.4348851872674264, 0.09694917142137004, 0.09694917142137004, 0.09694917142137004, 0.09694917142137004, 0.09694917142137004, 0.5816950285282202, 0.10973357903640622, 0.10973357903640622, 0.10973357903640622, 0.10973357903640622, 0.10973357903640622, 0.5486678951820311, 0.2890141854361935, 0.05481303516893326, 0.3338612142107753, 0.09467706074633926, 0.17938811509832703, 0.014949009591527252, 0.029898019183054503, 0.00498300319717575, 0.384440511916481, 0.23798698356734538, 0.05797118830486618, 0.06407341865274684, 0.12204460695761302, 0.08543122487032911, 0.048817842783045205, 0.019199255340084046, 0.83836748318367, 0.006399751780028016, 0.019199255340084046, 0.08959652492039222, 0.006399751780028016, 0.006399751780028016, 0.025599007120112063, 0.9335715594255958, 0.014145023627660541, 0.014145023627660541, 0.014145023627660541, 0.014145023627660541, 0.3595006964945203, 0.5013577280842499, 0.038864940161569765, 0.038864940161569765, 0.03497844614541279, 0.005829741024235465, 0.019432470080784883, 0.05412302594314253, 0.811845389147138, 0.07216403459085671, 0.036082017295428356, 0.018041008647714178, 0.018041008647714178, 0.8228398878458255, 0.037401813083901156, 0.037401813083901156, 0.037401813083901156, 0.037401813083901156, 0.12215170123795924, 0.12215170123795924, 0.12215170123795924, 0.12215170123795924, 0.12215170123795924, 0.48860680495183695, 0.09389102279034256, 0.42612079574078543, 0.0072223863684878885, 0.08666863642185467, 0.22389397742312456, 0.07944625005336678, 0.050556704579415225, 0.036111931842439444, 0.1757249578555723, 0.1757249578555723, 0.1757249578555723, 0.1757249578555723, 0.1757249578555723, 0.1757249578555723, 0.07373137676255463, 0.07373137676255463, 0.07373137676255463, 0.07373137676255463, 0.7373137676255463, 0.21246578029453728, 0.023607308921615253, 0.6137900319619966, 0.023607308921615253, 0.023607308921615253, 0.09442923568646101, 0.0638970027686947, 0.0638970027686947, 0.7667640332243364, 0.0638970027686947, 0.0638970027686947, 0.1160281097512173, 0.1160281097512173, 0.1160281097512173, 0.1160281097512173, 0.1160281097512173, 0.5801405487560864, 0.056036737207646466, 0.056036737207646466, 0.056036737207646466, 0.056036737207646466, 0.728477583699404, 0.056036737207646466, 0.044408119016507486, 0.022204059508253743, 0.06661217852476123, 0.022204059508253743, 0.022204059508253743, 0.8215502018053885, 0.3419531938720303, 0.3774150065698705, 0.060791678910583166, 0.09878647822969765, 0.035461812697840184, 0.04306077256166308, 0.04306077256166308, 0.4190554366193173, 0.3081289975142039, 0.02465031980113631, 0.14173933885653378, 0.006162579950284078, 0.0554632195525567, 0.012325159900568155, 0.02465031980113631, 0.6096618363407788, 0.18758825733562426, 0.09770221736230429, 0.03517279825042954, 0.003908088694492172, 0.023448532166953032, 0.04298897563941389, 0.09231495525794929, 0.09231495525794929, 0.09231495525794929, 0.09231495525794929, 0.09231495525794929, 0.646204686805645, 0.0405850359006055, 0.81170071801211, 0.0405850359006055, 0.0405850359006055, 0.0405850359006055, 0.047100290117235155, 0.047100290117235155, 0.047100290117235155, 0.8478052221102327, 0.047100290117235155, 0.6450332902891531, 0.07393662220705707, 0.11727878005257329, 0.06883754481346693, 0.071387083510262, 0.0076486160903852145, 0.015297232180770429, 0.7877765167793115, 0.04258251442050332, 0.11710191465638414, 0.02129125721025166, 0.01064562860512583, 0.01064562860512583, 0.01064562860512583, 0.020906870626877794, 0.020906870626877794, 0.9199023075826229, 0.020906870626877794, 0.020906870626877794, 0.27182884849554745, 0.39066113199086877, 0.062386948835043685, 0.16190898626237527, 0.046047509854437, 0.01188322834953213, 0.05347452757289459, 0.0014854035436915162, 0.1966367554323265, 0.09831837771616325, 0.09831837771616325, 0.09831837771616325, 0.09831837771616325, 0.49159188858081626, 0.147316471165525, 0.04910549038850833, 0.7120296106333708, 0.024552745194254166, 0.024552745194254166, 0.024552745194254166, 0.024552745194254166, 0.09651407712448655, 0.06434271808299104, 0.03217135904149552, 0.7399412579543969, 0.03217135904149552, 0.03217135904149552, 0.6766324803760168, 0.0631523648350949, 0.08570678084762878, 0.1443482624802169, 0.013532649607520333, 0.013532649607520333, 0.009021766405013556, 0.004510883202506778, 0.644665860086805, 0.07036845655877097, 0.10441770973236983, 0.15208666417540823, 0.009079800846293028, 0.011349751057866285, 0.009079800846293028, 0.002269950211573257, 0.1172888766371024, 0.538148963393764, 0.006899345684535435, 0.05519476547628348, 0.006899345684535435, 0.269074481696882, 0.006899345684535435, 0.042443332520767936, 0.042443332520767936, 0.042443332520767936, 0.042443332520767936, 0.8488666504153587, 0.055656886622872, 0.055656886622872, 0.055656886622872, 0.779196412720208, 0.055656886622872, 0.1552087004235132, 0.08407137939606965, 0.1681427587921393, 0.4914942180077918, 0.0776043502117566, 0.0129340583686261, 0.01940108755293915, 0.058846171050448245, 0.058846171050448245, 0.058846171050448245, 0.058846171050448245, 0.11769234210089649, 0.7061540526053789, 0.06045423123484714, 0.06045423123484714, 0.06045423123484714, 0.06045423123484714, 0.06045423123484714, 0.7859050060530128, 0.043678837536311785, 0.043678837536311785, 0.043678837536311785, 0.043678837536311785, 0.043678837536311785, 0.829897913189924, 0.07068260833426572, 0.07068260833426572, 0.07068260833426572, 0.07068260833426572, 0.07068260833426572, 0.7068260833426573, 0.4072310393676756, 0.24657238037663054, 0.11684266108439637, 0.09106854466872069, 0.07130838875003602, 0.031788076912666656, 0.032647214126522515, 0.0017182744277117112, 0.3267858065202232, 0.04357144086936309, 0.10892860217340773, 0.04357144086936309, 0.021785720434681546, 0.021785720434681546, 0.41392868825894935, 0.14730025057837692, 0.058920100231350775, 0.029460050115675387, 0.08838015034702616, 0.029460050115675387, 0.058920100231350775, 0.5892010023135077, 0.3172726585004075, 0.23904104407564947, 0.17167493165433007, 0.13255912444195106, 0.07823161442475801, 0.02607720480825267, 0.028250305208940393, 0.0021731004006877224, 0.4057508896119706, 0.23461786134025508, 0.1462911370709826, 0.08004609386902821, 0.09108693440268727, 0.0027602101334147657, 0.008280630400244297, 0.030362311467562426, 0.32393524859101785, 0.09329335159421315, 0.2410078249517173, 0.13475706341386343, 0.08551890562802872, 0.10106779756039758, 0.010365927954912571, 0.010365927954912571, 0.8984415667738027, 0.012654106574278911, 0.012654106574278911, 0.012654106574278911, 0.06327053287139456, 0.09295331808719143, 0.09295331808719143, 0.09295331808719143, 0.09295331808719143, 0.09295331808719143, 0.5577199085231486, 0.09295331808719143, 0.21670373697147152, 0.021670373697147152, 0.021670373697147152, 0.021670373697147152, 0.17336298957717722, 0.4767482213372373, 0.08668149478858861, 0.426536903607551, 0.2994833578521103, 0.027225759804737296, 0.018150506536491532, 0.07260202614596613, 0.036301013072983064, 0.1270535457554407, 0.09524330976749666, 0.09524330976749666, 0.09524330976749666, 0.09524330976749666, 0.09524330976749666, 0.5714598586049799, 0.8672840320315391, 0.04129923962054949, 0.04129923962054949, 0.04129923962054949, 0.04129923962054949, 0.04129923962054949, 0.11192847586500632, 0.11192847586500632, 0.33578542759501895, 0.11192847586500632, 0.11192847586500632, 0.33578542759501895, 0.17570057999899322, 0.17570057999899322, 0.17570057999899322, 0.17570057999899322, 0.17570057999899322, 0.17570057999899322, 0.06403281404808456, 0.03201640702404228, 0.09604922107212685, 0.06403281404808456, 0.6723445475048879, 0.03201640702404228, 0.10542512616684684, 0.05271256308342342, 0.05271256308342342, 0.05271256308342342, 0.737975883167928, 0.03832173439730945, 0.03832173439730945, 0.03832173439730945, 0.03832173439730945, 0.8430781567408079, 0.17694456650118598, 0.17694456650118598, 0.17694456650118598, 0.17694456650118598, 0.17694456650118598, 0.35388913300237196, 0.33704373306418994, 0.08114015795989758, 0.14667643938904565, 0.3620099355133892, 0.03432852836764898, 0.018724651836899444, 0.015603876530749535, 0.003120775306149907, 0.1596783366910943, 0.1596783366910943, 0.1596783366910943, 0.1596783366910943, 0.1596783366910943, 0.3193566733821886, 0.053660516781327924, 0.053660516781327924, 0.053660516781327924, 0.8049077517199188, 0.053660516781327924, 0.07365137024037055, 0.07365137024037055, 0.07365137024037055, 0.07365137024037055, 0.7365137024037055, 0.15045415255584627, 0.09027249153350776, 0.17452681696478167, 0.012036332204467702, 0.2708174746005233, 0.012036332204467702, 0.282853806804991, 0.006018166102233851, 0.13560259631607066, 0.13560259631607066, 0.13560259631607066, 0.13560259631607066, 0.13560259631607066, 0.5424103852642826, 0.04768361489154381, 0.04768361489154381, 0.8106214531562448, 0.04768361489154381, 0.04768361489154381, 0.07014701560941088, 0.07014701560941088, 0.7716171717035196, 0.07014701560941088, 0.07014701560941088, 0.06408681086832288, 0.06408681086832288, 0.7690417304198746, 0.06408681086832288, 0.06408681086832288], \"Term\": [\"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"actor_tutor\", \"actor_tutor\", \"actor_tutor\", \"actor_tutor\", \"actor_tutor\", \"ad\", \"ad\", \"ad\", \"ad\", \"ad\", \"ad\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adamenn\", \"adamenn\", \"adamenn\", \"adamenn\", \"adamenn\", \"adamenn\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptor\", \"adaptor\", \"adaptor\", \"adaptor\", \"adaptor\", \"adaptor_grammar\", \"adaptor_grammar\", \"adaptor_grammar\", \"adaptor_grammar\", \"adaptor_grammar\", \"aew\", \"aew\", \"aew\", \"aew\", \"aew\", \"agglomeration\", \"agglomeration\", \"agglomeration\", \"agglomeration\", \"agglomeration\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"align\", \"align\", \"align\", \"align\", \"align\", \"align\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alphanet\", \"alphanet\", \"alphanet\", \"alphanet\", \"alphanet\", \"alphanet\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amplitude\", \"amplitude\", \"amplitude\", \"amplitude\", \"amplitude\", \"amplitude\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appendix\", \"appendix\", \"appendix\", \"appendix\", \"appendix\", \"appendix\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"arrhythmia\", \"assemble\", \"assemble\", \"assemble\", \"assemble\", \"assemble\", \"attractive\", \"attractive\", \"attractive\", \"attractive\", \"attractive\", \"attractive\", \"attractiveness\", \"attractiveness\", \"attractiveness\", \"attractiveness\", \"attractiveness\", \"attractiveness\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"averaged_face\", \"averaged_face\", \"averaged_face\", \"averaged_face\", \"averaged_face\", \"averaged_face\", \"averageness\", \"averageness\", \"averageness\", \"averageness\", \"averageness\", \"averageness\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base_learner\", \"base_learner\", \"base_learner\", \"base_learner\", \"base_learner\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"beak\", \"beak\", \"beak\", \"beak\", \"beak\", \"bfgs\", \"bfgs\", \"bfgs\", \"bfgs\", \"bfgs\", \"bird\", \"bird\", \"bird\", \"bird\", \"bird\", \"board\", \"board\", \"board\", \"board\", \"board\", \"bond\", \"bond\", \"bond\", \"bond\", \"bond\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"burst\", \"burst\", \"burst\", \"burst\", \"burst\", \"burst\", \"calibrate\", \"calibrate\", \"calibrate\", \"calibrate\", \"calibrate\", \"calibrate\", \"calibrated_surrogate\", \"calibrated_surrogate\", \"calibrated_surrogate\", \"calibrated_surrogate\", \"calibrated_surrogate\", \"calibrated_surrogate\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cca\", \"cca\", \"cca\", \"cca\", \"cca\", \"cca\", \"cdn\", \"cdn\", \"cdn\", \"cdn\", \"cdn\", \"cdn\", \"celi\", \"celi\", \"celi\", \"celi\", \"celi\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"char\", \"char\", \"char\", \"char\", \"char\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"chess\", \"chess\", \"chess\", \"chess\", \"chess\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn_message\", \"cnn_message\", \"cnn_message\", \"cnn_message\", \"cnn_message\", \"cnn_message\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"codon\", \"codon\", \"codon\", \"codon\", \"codon\", \"codon\", \"combining_rule\", \"combining_rule\", \"combining_rule\", \"combining_rule\", \"combining_rule\", \"combining_rule\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"conquer\", \"conquer\", \"conquer\", \"conquer\", \"conquer\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex_calibrate\", \"convex_calibrate\", \"convex_calibrate\", \"convex_calibrate\", \"convex_calibrate\", \"convex_calibrate\", \"corollary\", \"corollary\", \"corollary\", \"corollary\", \"corollary\", \"corollary\", \"coughlan\", \"coughlan\", \"coughlan\", \"coughlan\", \"coughlan\", \"cps\", \"cps\", \"cps\", \"cps\", \"cps\", \"crf\", \"crf\", \"crf\", \"crf\", \"crf\", \"crf\", \"ctw\", \"ctw\", \"ctw\", \"ctw\", \"ctw\", \"ctw\", \"cumulant\", \"cumulant\", \"cumulant\", \"cumulant\", \"cumulant\", \"cvpr\", \"cvpr\", \"cvpr\", \"cvpr\", \"cvpr\", \"cvpr\", \"cvpr\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"daubechie\", \"daubechie\", \"daubechie\", \"daubechie\", \"daubechie\", \"ddp\", \"ddp\", \"ddp\", \"ddp\", \"ddp\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"deep_structure\", \"deep_structure\", \"deep_structure\", \"deep_structure\", \"deep_structure\", \"deep_structure\", \"deeplab\", \"deeplab\", \"deeplab\", \"deeplab\", \"deeplab\", \"deeplab\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"degradation\", \"degradation\", \"degradation\", \"degradation\", \"degradation\", \"degradation\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"dene\", \"dene\", \"dene\", \"dene\", \"dene\", \"dene\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"deque\", \"deque\", \"deque\", \"deque\", \"deque\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dispersion\", \"dispersion\", \"dispersion\", \"dispersion\", \"dispersion\", \"distributed_associative\", \"distributed_associative\", \"distributed_associative\", \"distributed_associative\", \"distributed_associative\", \"distributed_associative\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence_estimate\", \"divergence_estimate\", \"divergence_estimate\", \"divergence_estimate\", \"divergence_estimate\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"dominate\", \"dominate\", \"dominate\", \"dominate\", \"dominate\", \"dominate\", \"dominate\", \"dominate\", \"dpf\", \"dpf\", \"dpf\", \"dpf\", \"dpf\", \"dpf\", \"drain\", \"drain\", \"drain\", \"drain\", \"drain\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"ea\", \"ea\", \"ea\", \"ea\", \"ea\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"electrosensory\", \"electrosensory\", \"electrosensory\", \"electrosensory\", \"electrosensory\", \"electrosensory\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"eod\", \"eod\", \"eod\", \"eod\", \"eod\", \"eod\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"eru\", \"eru\", \"eru\", \"eru\", \"eru\", \"eru\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"etc_strategy\", \"etc_strategy\", \"etc_strategy\", \"etc_strategy\", \"etc_strategy\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus_eus\", \"eus_eus\", \"eus_eus\", \"eus_eus\", \"eus_eus\", \"eus_eus\", \"evoi\", \"evoi\", \"evoi\", \"evoi\", \"evoi\", \"evoi\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"expected_utility\", \"expected_utility\", \"expected_utility\", \"expected_utility\", \"expected_utility\", \"expected_utility\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"facial_attractiveness\", \"facial_attractiveness\", \"facial_attractiveness\", \"facial_attractiveness\", \"facial_attractiveness\", \"facial_attractiveness\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"feasibility_region\", \"feasibility_region\", \"feasibility_region\", \"feasibility_region\", \"feasibility_region\", \"feasibility_region\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fisher_information\", \"fisher_information\", \"fisher_information\", \"fisher_information\", \"fisher_information\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"fritzke\", \"fritzke\", \"fritzke\", \"fritzke\", \"fritzke\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"full_capacity\", \"full_capacity\", \"full_capacity\", \"full_capacity\", \"full_capacity\", \"full_capacity\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"fvs\", \"fvs\", \"fvs\", \"fvs\", \"fvs\", \"fvs\", \"fvss\", \"fvss\", \"fvss\", \"fvss\", \"fvss\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gcld\", \"gcld\", \"gcld\", \"gcld\", \"gcld\", \"genealogy\", \"genealogy\", \"genealogy\", \"genealogy\", \"genealogy\", \"geodesic\", \"geodesic\", \"geodesic\", \"geodesic\", \"geodesic\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gpc\", \"gplvm\", \"gplvm\", \"gplvm\", \"gplvm\", \"gplvm\", \"gplvm\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"greedy_importance\", \"greedy_importance\", \"greedy_importance\", \"greedy_importance\", \"greedy_importance\", \"hgf\", \"hgf\", \"hgf\", \"hgf\", \"hgf\", \"highlighting\", \"highlighting\", \"highlighting\", \"highlighting\", \"highlighting\", \"highlighting\", \"hmc\", \"hmc\", \"hmc\", \"hmc\", \"hmc\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human_rater\", \"human_rater\", \"human_rater\", \"human_rater\", \"human_rater\", \"human_rater\", \"ievel\", \"ievel\", \"ievel\", \"ievel\", \"ievel\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imitation\", \"imitation\", \"imitation\", \"imitation\", \"imitation\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"inhibition\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"kavg\", \"kavg\", \"kavg\", \"kavg\", \"kavg\", \"kde\", \"kde\", \"kde\", \"kde\", \"kde\", \"kde\", \"keypoint\", \"keypoint\", \"keypoint\", \"keypoint\", \"keypoint\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"lakoff\", \"lakoff\", \"lakoff\", \"lakoff\", \"lakoff\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"lateral_connection\", \"lateral_connection\", \"lateral_connection\", \"lateral_connection\", \"lateral_connection\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda_moment\", \"lda_moment\", \"lda_moment\", \"lda_moment\", \"lda_moment\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lift\", \"lift\", \"lift\", \"lift\", \"lift\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"lnp\", \"lnp\", \"lnp\", \"lnp\", \"lnp\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log_det\", \"log_det\", \"log_det\", \"log_det\", \"log_det\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lsh\", \"lsh\", \"lsh\", \"lsh\", \"lsh\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"marginalization\", \"marginalization\", \"marginalization\", \"marginalization\", \"marginalization\", \"marginalization\", \"martinetz\", \"martinetz\", \"martinetz\", \"martinetz\", \"martinetz\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"max_product\", \"max_product\", \"max_product\", \"max_product\", \"max_product\", \"maze\", \"maze\", \"maze\", \"maze\", \"maze\", \"mcv\", \"mcv\", \"mcv\", \"mcv\", \"mcv\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median_fullset\", \"median_fullset\", \"median_fullset\", \"median_fullset\", \"median_fullset\", \"memorized\", \"memorized\", \"memorized\", \"memorized\", \"memorized\", \"memorized\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message_estimator\", \"message_estimator\", \"message_estimator\", \"message_estimator\", \"message_estimator\", \"message_estimator\", \"message_passe\", \"message_passe\", \"message_passe\", \"message_passe\", \"message_passe\", \"message_passe\", \"message_passe\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metzner\", \"metzner\", \"metzner\", \"metzner\", \"metzner\", \"metzner\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"mono\", \"mono\", \"mono\", \"mono\", \"mono\", \"mono\", \"moonsu\", \"moonsu\", \"moonsu\", \"moonsu\", \"moonsu\", \"moonsu\", \"moonsu_hr\", \"moonsu_hr\", \"moonsu_hr\", \"moonsu_hr\", \"moonsu_hr\", \"moonsu_hr\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"moving_target\", \"moving_target\", \"moving_target\", \"moving_target\", \"moving_target\", \"multiple_source\", \"multiple_source\", \"multiple_source\", \"multiple_source\", \"multiple_source\", \"multiple_source\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"name\", \"name\", \"name\", \"name\", \"name\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neurochess\", \"neurochess\", \"neurochess\", \"neurochess\", \"neurochess\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"nmax\", \"nmax\", \"nmax\", \"nmax\", \"nmax\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non_rigid\", \"non_rigid\", \"non_rigid\", \"non_rigid\", \"non_rigid\", \"non_rigid\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonterminal\", \"nonterminal\", \"nonterminal\", \"nonterminal\", \"nonterminal\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nrsfm\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"nsr\", \"null\", \"null\", \"null\", \"null\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"occlusion\", \"occlusion\", \"occlusion\", \"occlusion\", \"occlusion\", \"occlusion\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal_recommendation\", \"optimal_recommendation\", \"optimal_recommendation\", \"optimal_recommendation\", \"optimal_recommendation\", \"optimal_recommendation\", \"optimise\", \"optimise\", \"optimise\", \"optimise\", \"optimise\", \"optimise\", \"option\", \"option\", \"option\", \"option\", \"option\", \"option\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"participant\", \"participant\", \"participant\", \"participant\", \"participant\", \"participant\", \"participant\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pcfg\", \"pcfg\", \"pcfg\", \"pcfg\", \"pcfg\", \"pcp\", \"pcp\", \"pcp\", \"pcp\", \"pcp\", \"pddp\", \"pddp\", \"pddp\", \"pddp\", \"pddp\", \"person\", \"person\", \"person\", \"person\", \"person\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phonology\", \"phonology\", \"phonology\", \"phonology\", \"phonology\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pilco\", \"pilco\", \"pilco\", \"pilco\", \"pilco\", \"pld\", \"pld\", \"pld\", \"pld\", \"pld\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"ppga\", \"ppga\", \"ppga\", \"ppga\", \"ppga\", \"pred\", \"pred\", \"pred\", \"pred\", \"pred\", \"pred\", \"predmap\", \"predmap\", \"predmap\", \"predmap\", \"predmap\", \"predmap\", \"predpd\", \"predpd\", \"predpd\", \"predpd\", \"predpd\", \"predpd\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"private\", \"private\", \"private\", \"private\", \"private\", \"privileged\", \"privileged\", \"privileged\", \"privileged\", \"privileged\", \"privileged_information\", \"privileged_information\", \"privileged_information\", \"privileged_information\", \"privileged_information\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processor\", \"processor\", \"processor\", \"processor\", \"processor\", \"processor\", \"production\", \"production\", \"production\", \"production\", \"production\", \"psdp\", \"psdp\", \"psdp\", \"psdp\", \"psdp\", \"pylon\", \"pylon\", \"pylon\", \"pylon\", \"pylon\", \"pyramidal_cell\", \"pyramidal_cell\", \"pyramidal_cell\", \"pyramidal_cell\", \"pyramidal_cell\", \"pyramidal_cell\", \"pyramidal_cell\", \"qde\", \"qde\", \"qde\", \"qde\", \"qde\", \"qde\", \"qp\", \"qp\", \"qp\", \"qp\", \"qp\", \"qp\", \"qp_rl\", \"qp_rl\", \"qp_rl\", \"qp_rl\", \"qp_rl\", \"qp_rl\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantize\", \"quantize\", \"quantize\", \"quantize\", \"quantize\", \"quantize\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rater\", \"rater\", \"rater\", \"rater\", \"rater\", \"rater\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rdn\", \"rdn\", \"rdn\", \"rdn\", \"rdn\", \"rdn\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"readout\", \"readout\", \"readout\", \"readout\", \"readout\", \"receptor_afferent\", \"receptor_afferent\", \"receptor_afferent\", \"receptor_afferent\", \"receptor_afferent\", \"receptor_afferent\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"recording\", \"recording\", \"recording\", \"recording\", \"recording\", \"recording\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regularized_distance\", \"regularized_distance\", \"regularized_distance\", \"regularized_distance\", \"regularized_distance\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"reservoir\", \"reservoir\", \"reservoir\", \"reservoir\", \"reservoir\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"restricted_capacity\", \"restricted_capacity\", \"restricted_capacity\", \"restricted_capacity\", \"restricted_capacity\", \"restricted_capacity\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythmic\", \"rhythmic\", \"rhythmic\", \"rhythmic\", \"rhythmic\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"rmgpc\", \"rmgpc\", \"rmgpc\", \"rmgpc\", \"rmgpc\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rohwer\", \"rohwer\", \"rohwer\", \"rohwer\", \"rohwer\", \"rollout\", \"rollout\", \"rollout\", \"rollout\", \"rollout\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rq\", \"rq\", \"rq\", \"rq\", \"rq\", \"rq\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape_descriptor\", \"shape_descriptor\", \"shape_descriptor\", \"shape_descriptor\", \"shape_descriptor\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"simd\", \"simd\", \"simd\", \"simd\", \"simd\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"slg\", \"slg\", \"slg\", \"slg\", \"slg\", \"sne\", \"sne\", \"sne\", \"sne\", \"sne\", \"soc_constraint\", \"soc_constraint\", \"soc_constraint\", \"soc_constraint\", \"soc_constraint\", \"soc_constraint\", \"socp\", \"socp\", \"socp\", \"socp\", \"socp\", \"socp\", \"socp_ms\", \"socp_ms\", \"socp_ms\", \"socp_ms\", \"socp_ms\", \"socp_ms\", \"socp_relaxation\", \"socp_relaxation\", \"socp_relaxation\", \"socp_relaxation\", \"socp_relaxation\", \"socp_relaxation\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"statistical_test\", \"statistical_test\", \"statistical_test\", \"statistical_test\", \"statistical_test\", \"stft\", \"stft\", \"stft\", \"stft\", \"stft\", \"stft\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"strictly_dominate\", \"strictly_dominate\", \"strictly_dominate\", \"strictly_dominate\", \"strictly_dominate\", \"strictly_dominate\", \"subdifferential\", \"subdifferential\", \"subdifferential\", \"subdifferential\", \"subdifferential\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"subset_ranke\", \"subset_ranke\", \"subset_ranke\", \"subset_ranke\", \"subset_ranke\", \"subset_ranke\", \"suffix\", \"suffix\", \"suffix\", \"suffix\", \"suffix\", \"suffix\", \"surrogate\", \"surrogate\", \"surrogate\", \"surrogate\", \"surrogate\", \"surrogate\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"tda\", \"tda\", \"tda\", \"tda\", \"tda\", \"tda\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"temper\", \"temper\", \"temper\", \"temper\", \"temper\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"tier\", \"tier\", \"tier\", \"tier\", \"tier\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time_warpe\", \"time_warpe\", \"time_warpe\", \"time_warpe\", \"time_warpe\", \"time_warpe\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"tuning_curve\", \"tuning_curve\", \"tuning_curve\", \"tuning_curve\", \"tuning_curve\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unitary\", \"unitary\", \"unitary\", \"unitary\", \"unitary\", \"unitary\", \"unitary_matrice\", \"unitary_matrice\", \"unitary_matrice\", \"unitary_matrice\", \"unitary_matrice\", \"unitary_matrice\", \"urnn\", \"urnn\", \"urnn\", \"urnn\", \"urnn\", \"urnn\", \"urnns\", \"urnns\", \"urnns\", \"urnns\", \"urnns\", \"urnns\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"vgg\", \"vgg\", \"vgg\", \"vgg\", \"vgg\", \"vgg\", \"vgg\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visualisation\", \"visualisation\", \"visualisation\", \"visualisation\", \"visualisation\", \"visualisation\", \"visualization\", \"visualization\", \"visualization\", \"visualization\", \"visualization\", \"visualization\", \"voc\", \"voc\", \"voc\", \"voc\", \"voc\", \"voc\", \"voc_extra\", \"voc_extra\", \"voc_extra\", \"voc_extra\", \"voc_extra\", \"voc_extra\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"voxel\", \"voxel\", \"voxel\", \"voxel\", \"voxel\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"weakly_electric\", \"weakly_electric\", \"weakly_electric\", \"weakly_electric\", \"weakly_electric\", \"weakly_electric\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted_combining\", \"weighted_combining\", \"weighted_combining\", \"weighted_combining\", \"weighted_combining\", \"weighted_combining\", \"weighted_matching\", \"weighted_matching\", \"weighted_matching\", \"weighted_matching\", \"weighted_matching\", \"wheeler\", \"wheeler\", \"wheeler\", \"wheeler\", \"wheeler\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word_list\", \"word_list\", \"word_list\", \"word_list\", \"word_list\", \"word_list\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"yqt\", \"yqt\", \"yqt\", \"yqt\", \"yqt\", \"zd\", \"zd\", \"zd\", \"zd\", \"zd\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 4, 6, 2, 7, 8, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el99371402293504234567435195691\", ldavis_el99371402293504234567435195691_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el99371402293504234567435195691\", ldavis_el99371402293504234567435195691_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el99371402293504234567435195691\", ldavis_el99371402293504234567435195691_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.087639  0.026779       1        1  31.393005\n",
       "2      0.055192 -0.027987       2        1  20.235450\n",
       "3      0.026125  0.004406       3        1  12.957076\n",
       "5      0.018279 -0.053168       4        1  12.461287\n",
       "1      0.004910  0.017987       5        1  11.176085\n",
       "6     -0.034666  0.044608       6        1   5.532799\n",
       "7     -0.045516  0.001491       7        1   4.619842\n",
       "4     -0.111962 -0.014116       8        1   1.624456, topic_info=         Term        Freq       Total Category  logprob  loglift\n",
       "418   network  518.000000  518.000000  Default  30.0000  30.0000\n",
       "305     image  569.000000  569.000000  Default  29.0000  29.0000\n",
       "429    object  202.000000  202.000000  Default  28.0000  28.0000\n",
       "2448   policy  320.000000  320.000000  Default  27.0000  27.0000\n",
       "82       cell  249.000000  249.000000  Default  26.0000  26.0000\n",
       "...       ...         ...         ...      ...      ...      ...\n",
       "2509   target    4.126839  162.269700   Topic8  -6.6746   0.4482\n",
       "946     learn    4.708374  887.372944   Topic8  -6.5427  -1.1189\n",
       "690    vector    4.299822  385.879587   Topic8  -6.6335  -0.3770\n",
       "1442    spike    3.890292  156.256060   Topic8  -6.7336   0.4270\n",
       "573       set    4.133838  959.519700   Topic8  -6.6729  -1.3272\n",
       "\n",
       "[558 rows x 6 columns], token_table=      Topic      Freq    Term\n",
       "term                         \n",
       "716       1  0.435704  action\n",
       "716       2  0.458338  action\n",
       "716       3  0.011317  action\n",
       "716       4  0.079219  action\n",
       "716       5  0.005658  action\n",
       "...     ...       ...     ...\n",
       "5073      1  0.064087      zd\n",
       "5073      2  0.064087      zd\n",
       "5073      3  0.769042      zd\n",
       "5073      4  0.064087      zd\n",
       "5073      5  0.064087      zd\n",
       "\n",
       "[2538 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 4, 6, 2, 7, 8, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('../results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, '../results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Closing Notes\n",
    "\n",
    "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
    "\n",
    "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
    "\n",
    "** **\n",
    "#### References:\n",
    "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
    "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
    "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
    "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
    "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "7. http://palmetto.aksw.org/palmetto-webapp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
