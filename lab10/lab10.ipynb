{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7c9080-2e29-41d0-a3df-7f8a3bfb413c",
   "metadata": {},
   "source": [
    "# Lab 10 - Chatbot using PyTorch\n",
    "In this lab you will create a Chatbot using sequence to sequence models. The chatbot will be trained on movie scripts from the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), using an encoder and decoder architecture.\n",
    "\n",
    "This lab is heavily based on the [Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#load-preprocess-data) available on the PyTorch website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154117f3-da18-4eaf-a004-b982d62a5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==1.11.0+cu113 torchdata==0.3.0 torchtext==0.12.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "%pip install tqdm ipywidgets spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d59d4-f1da-4635-a674-becc78df704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e283cc-3b06-44bb-98d8-914a03ce32a1",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "The first step will be downloading and processing the corpus we will be working with.\n",
    "\n",
    "We will be using the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) to train the model on dialog line pairs.\n",
    "\n",
    "The corpus contains 220,579 conversational exchanges between 10,292 pairs of movie characters with 9,035 characters total, from 617 movies and 304,713 total utterances. This variety makes the dataset very diverse as far as tone and sentiment are concerned, which  makes it ideal for training a chatbot.\n",
    "\n",
    "The primary downside of the dataset is that it needs rather thorough processing and cleaning for use in chatbot training, but we will tackle it step by step.\n",
    "\n",
    "First, we need to download it. We'll use Python's `urllib` and `zipfile` libraries to quickly download the zip file and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f974ce7-3138-4b11-8fa1-7573693b2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def download_cornell_movie_dialogs(extract_dir = Path(\".\")):\n",
    "    # Download\n",
    "    URL = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "    zip_path, _ = urllib.request.urlretrieve(URL)\n",
    "\n",
    "    # Unzip\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(extract_dir)\n",
    "\n",
    "download_cornell_movie_dialogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94919ed2-919e-4439-a0c9-b2e4e7208b35",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "We can inspect what the data looks like originally pretty easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb867cd0-abb9-41c9-a6b6-a16a59ecb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"cornell movie-dialogs corpus\")\n",
    "\n",
    "def _print_x_lines(x, file):\n",
    "    with open(file, \"r\") as f:\n",
    "        for _ in range(x):\n",
    "            print(f.readline())\n",
    "\n",
    "_print_x_lines(10, DATA_PATH / \"movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0a81e-bb9b-414f-9a04-aea2a07e087b",
   "metadata": {},
   "source": [
    "As you can see, each line has:\n",
    "- a line ID at the start,\n",
    "- then the ID of the character saying the line,\n",
    "- the ID of the movie the line belongs to,\n",
    "- the character's name, and last but not least,\n",
    "- the actual line\n",
    "\n",
    "The file is essentially in csv format with `+++$+++` as the delimiter. As such, we'll process each line into a dictionary with those fields much more explicit for easier access later.\n",
    "\n",
    "The corpus additionally has a \"conversations\" file, which groups individual lines together into distinct conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c7ada-febd-4395-bbd9-bb3065d31274",
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_x_lines(10, DATA_PATH / \"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6e832-aa0e-4bf9-bdd9-876c4a5f7c42",
   "metadata": {},
   "source": [
    "We can observe that each conversation has the following format:\n",
    "- the ID of the first character involved in the conversation\n",
    "- the ID of the second character involved in the conversation\n",
    "- the ID of the movie the conversation takes place in\n",
    "- a list of line IDs included in the conversation.\n",
    "\n",
    "`+++$+++` is the delimiter once again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e175b-0fd7-49d8-a5c1-f19d4e1ccce5",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Having our data in its raw format is very inconvenient, so we'll convert everything to Python dictionaries for easy access to all the information we need during training.\n",
    "\n",
    "As far as lines are concerned, we will simply split them and conver them to Python dictionraries with the fields identified in the previous section as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3abf02-959b-4d30-9c71-a255c8193889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_line(line, field_names):\n",
    "    line = line.split(\" +++$+++ \")  # Delimiter\n",
    "    line = dict(zip(field_names, line))  # To dict using field_names as keys\n",
    "    return line\n",
    "\n",
    "def process_lines():\n",
    "    # Fields as they appear in the data\n",
    "    FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "    \n",
    "    lines = {}  # Map from line ID to line object\n",
    "    # Note the encoding, the dataset is not plain UTF-8\n",
    "    with open(DATA_PATH / \"movie_lines.txt\", \"r\", encoding=\"iso-8859-1\") as f:\n",
    "        for line in f.readlines():  # For each line\n",
    "            line_dict = _process_line(line, FIELDS)  # Process it\n",
    "            lines[line_dict[\"lineID\"]] = line_dict  # Store it according to its ID\n",
    "\n",
    "    return lines\n",
    "\n",
    "movie_lines = process_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7b901-aa6d-4b43-b29d-77fe5f89f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_lines[\"L1045\"])\n",
    "print(movie_lines[\"L1044\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810ee51-817b-4d07-a18a-7e9c0983742a",
   "metadata": {},
   "source": [
    "We'll process conversations in a very similar way. The only thing we'll do differently is that we will match each Line ID to its actual line object and store it to each extracted conversation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548796d7-8436-4fd7-98fc-98eefc04dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_conversation(convo, field_names):\n",
    "    \n",
    "    convo = convo.split(\" +++$+++ \")\n",
    "    convo = dict(zip(field_names, convo))\n",
    "    \n",
    "    convo[\"lineIDs\"] = eval(convo[\"lineIDs\"])  # Convert to Python list\n",
    "    # Fetch actual line objects\n",
    "    convo[\"lines\"] = [movie_lines[line_id] for line_id in convo[\"lineIDs\"]]\n",
    "    \n",
    "    return convo\n",
    "    \n",
    "def process_conversations():\n",
    "    FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"lineIDs\"]\n",
    "    \n",
    "    convos = []\n",
    "    with open(DATA_PATH / \"movie_conversations.txt\", \"r\", encoding=\"iso-8859-1\") as f:\n",
    "        for convo in f.readlines():\n",
    "            convo_dict = _process_conversation(convo, FIELDS)\n",
    "            convos.append(convo_dict)\n",
    "            \n",
    "    return convos\n",
    "\n",
    "movie_conversations = process_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766098e-e8ee-45f9-a294-e1c288134d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed0593-5505-48d2-be2d-65fd51a1a05b",
   "metadata": {},
   "source": [
    "Finally, we'll extract line pairs, in a sort of answer and response format from the corpus. We'll use the conversation information to extract the line pairs.\n",
    "\n",
    "Then, we'll save these line pairs in a TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf33e4-9fb7-4eef-830a-2db00812d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_line_pairs(convos):\n",
    "    pairs = []\n",
    "    for convo in convos:\n",
    "        # Ignore last line as it has no line to be paired with\n",
    "        for i, input_line in enumerate(convo[\"lines\"][:-1]):\n",
    "            input_line = input_line[\"text\"].strip()\n",
    "            target_line = convo[\"lines\"][i+1][\"text\"].strip()\n",
    "            \n",
    "            if input_line and target_line:  # It's possible either line is empty\n",
    "                pairs.append((input_line, target_line))\n",
    "    return pairs\n",
    "\n",
    "# Example\n",
    "_get_line_pairs([movie_conversations[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c2120-8bd9-48ea-a600-ddb47a77e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "\n",
    "output_file = DATA_PATH / \"formatted_movie_lines.tsv\"\n",
    "delimiter = str(codecs.decode(\"\\t\", \"unicode_escape\"))  # HACK to quickly get an actual tab character\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=delimiter, lineterminator=\"\\n\")\n",
    "    for pair in _get_line_pairs(movie_conversations):\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efe9a0-b7a4-4d31-b430-8c8db46f3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TSV file\n",
    "_print_x_lines(2, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef8342-0410-46e4-822d-85d3bae2acb9",
   "metadata": {},
   "source": [
    "If you're using SageMaker Studio Lab, you can open the actual `.tsv` file to inspect it in a more visual format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5713a-c9bf-4985-8aed-aafede9c0891",
   "metadata": {},
   "source": [
    "### Standard NLP data processing\n",
    "As the title of this subsection suggests, the next step will be applying some good old NLP data processing to the lines.\n",
    "\n",
    "So far, we processed the raw dataset into pairs of sentences that we know are part of a conversation. We will now apply standard processing pipelines to each line to facilitate the application of the models we'll apply later.\n",
    "\n",
    "As usual, this parimarily involves tokenisation and creating a vocabulary.\n",
    "\n",
    "First let's define the tokenizer. We'll use SpaCy as in previous labs but with a couple of modifications, such as removing punctuation and lowercasing everything. Also as you might have noticed in the previous steps, there were some tricks related to encodings when reading in the corpus or saving it (had to use full utf-8). We want only plain latin characters (ASCII) to remain, so we will be converting to that too and stripping out everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915df85b-75fb-437a-9cff-fa4dc9e0012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class SpacyTokenizer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "        \n",
    "    # Turn a Unicode string to plain ASCII, thanks to\n",
    "    # https://stackoverflow.com/a/518232/2809427\n",
    "    def _unicode_to_ascii(self, sentence):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', sentence)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "        \n",
    "    def tokenize_sentence(self, sentence):\n",
    "        # To ASCII and to lower\n",
    "        sentence = self._unicode_to_ascii(sentence.lower().strip())\n",
    "        # Tokenize\n",
    "        sentence = self.tokenizer(sentence)\n",
    "        # Remove extra puncutation\n",
    "        sentence = [re.sub(r\"[^a-zA-Z.'!?]+\", \"\", token) for token in sentence]\n",
    "        # Remove empty strings\n",
    "        sentence = [token for token in sentence if token]\n",
    "        return sentence\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input, list):\n",
    "            tokens = []\n",
    "            for text in input:\n",
    "                tokens.append(self.tokenize_sentence(text))\n",
    "            return tokens\n",
    "        elif isinstance(input, str):\n",
    "            return self.tokenize_sentence(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")\n",
    "\n",
    "# Example\n",
    "line = movie_lines[\"L197\"][\"text\"]\n",
    "print(f\"Before tokenization: {line}\")\n",
    "tokenizer = SpacyTokenizer()\n",
    "print(f\"After tokenization: {tokenizer(line)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1569578-b937-45b6-8825-d72dc80ac2c0",
   "metadata": {},
   "source": [
    "Now that we have our tokenizer, next step is to apply it to our line pairs and build a vocabulary from it.\n",
    "\n",
    "First, let's actually load our line pairs into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b04868-5df4-46f4-8cdb-e18a93e17d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "DATA_PATH = Path(\"cornell movie-dialogs corpus\")\n",
    "\n",
    "def _load_pair_data():\n",
    "    with open(DATA_PATH / \"formatted_movie_lines.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = list(csv.reader(f, delimiter=\"\\t\"))\n",
    "    return pairs\n",
    "        \n",
    "movie_line_pairs = _load_pair_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2dd2f-9fec-4007-a2d1-860f09ba121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_line_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a83ef-9f8a-4d4d-9d51-5527760ec62e",
   "metadata": {},
   "source": [
    "Before we continue, we'll trim our dataset to only include lines that are on the shorter end ($10$ words or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c444d17-490e-48a7-b336-379b6f1f7dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def filter_pairs(pairs, max_length: int):\n",
    "    tokenizer = SpacyTokenizer()\n",
    "    \n",
    "    def _keep_pair(pair):\n",
    "        return len(tokenizer(pair[0])) < max_length\\\n",
    "            and len(tokenizer(pair[1])) < max_length\n",
    "\n",
    "    out_pairs = []\n",
    "    for pair in tqdm(pairs):\n",
    "        if _keep_pair(pair):\n",
    "            out_pairs.append(pair)\n",
    "    return out_pairs\n",
    "\n",
    "print(f\"Original dataset size: {len(movie_line_pairs)} pairs\")\n",
    "trimmed_movie_line_pairs = filter_pairs(movie_line_pairs, MAX_LENGTH)\n",
    "print(f\"Trimmed dataset size: {len(trimmed_movie_line_pairs)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5944e73-0072-4053-81b4-d5f124da02d9",
   "metadata": {},
   "source": [
    "Next we'll use our usual Vocab building methods using torchtext to build our vocabulary.\n",
    "\n",
    "We'll also include a Beginning of Sentence (BOS) and End of Sentence (EOS) special tokens alongside our typical Uknown (UNK) and Padding (PAD) special tokens.\n",
    "\n",
    "Additionally, we'll only include words in our vocab that appear a minimum of $3$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc1efe-cd72-4371-8cb2-815fea214733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "MIN_FREQ = 3\n",
    "\n",
    "def _process_pairs_for_vocab(data):\n",
    "    for pairs in data:  # Add tokens from both lines in the pair\n",
    "        yield tokenizer(pairs[0]) + tokenizer(pairs[1])\n",
    "\n",
    "text_vocab = build_vocab_from_iterator(\n",
    "    _process_pairs_for_vocab(trimmed_movie_line_pairs),\n",
    "    specials=('<unk>', '<pad>', '<bos>', '<eos>'),\n",
    "    min_freq=MIN_FREQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aaf96-1fc0-4fd4-af90-58f2e5be28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in vocabulary: {len(text_vocab)}\")\n",
    "print(\"\\nFirst 20 tokens: \")\n",
    "print(text_vocab.get_itos()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdbb4-4e61-475e-84e5-af27656903d4",
   "metadata": {},
   "source": [
    "Next we'll filter our dataset again to only include pairs where both lines contain tokens that are found in our vocabulary, i.e. tokens that weren't filtered by our minimum frequency rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf8d3e-04e0-4fc0-96f5-1c37f903266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def filter_pairs(pairs, vocab):\n",
    "    tokenizer = SpacyTokenizer()    \n",
    "    vocab_tokens = set(vocab.get_itos())\n",
    "    \n",
    "    def _keep_pair(pair):\n",
    "        tokens = set(tokenizer(pair[0]) + tokenizer(pair[1]))\n",
    "        for token in tokens:\n",
    "            if token not in vocab_tokens:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    out_pairs = []\n",
    "    for pair in tqdm(pairs, desc=\"Trimming pairs based on vocab...\"):\n",
    "        if _keep_pair(pair):\n",
    "            out_pairs.append(pair)\n",
    "    return out_pairs\n",
    "\n",
    "print(f\"Original trimmed dataset size: {len(trimmed_movie_line_pairs)} pairs.\")\n",
    "final_movie_line_pairs = filter_pairs(trimmed_movie_line_pairs, text_vocab)\n",
    "print(f\"Final dataset size: {len(final_movie_line_pairs)} pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f2301-be64-4a93-a228-59c4e2da6af1",
   "metadata": {},
   "source": [
    "The final step is to define processing pipelines and data loaders.\n",
    "\n",
    "We want to apply the following transformations to our data:\n",
    "- For inputs (first of the two sentences in each pair):\n",
    "  1. Tokenize\n",
    "  2. Transform into vocabulary indices\n",
    "  3. Add the EOS token\n",
    "  4. Do the following:\n",
    "     1. Get the length of this tokenized sentence.\n",
    "     2. Pad this tokenized sentence to the maximum length of the batch and convert to a tensor. \n",
    "  - To this end, we'll use `input_transform_common` for steps 1 through 4. On top of that, we'll use `inputs_transform` for 4.2 and and `lengths_transform` for 4.1. To extract sentence lengths we'll define a `ToLengths` transform.\n",
    "\n",
    "- For outputs (second of the two sentences in each pair):\n",
    "  1. Tokenize\n",
    "  2. Transform into vocabulary indices\n",
    "  3. Add the EOS token\n",
    "  4. Do the following:\n",
    "     1. Get the maximum tokenized sentence length.\n",
    "     2. Pad this tokenized sentence to the maximum length of the batch and convert to a tensor.\n",
    "     3. Create a mask that has 0 for each token that is not padding, and 1 for each token that is.\n",
    "  - We'll use `output_transform_common` for steps 1 through 4, then `output_transform` for step 4.2 and `mask_transform` for step 4.3. For step 4.1 we'll use a tiny bit of Python code when we actually apply the transforms to extract this maximum length from the results of `output_transform_common`. For the mask transform we'll define a custom transformation to quickly do that.\n",
    "\n",
    "Applying this processing with typical torchtext and some custom transforms will lead to the first dimension of our batch being the actual batch size, and the second dimension being the tokens. However, for each time step, we want to be able to retrieve all the words for that time step in the batch easily, so we will also be donig a transpose in our actual `collate_batch` function.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq_batches.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb93142-097f-4085-a77e-5f8646dd8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "class ToLengths(torch.nn.Module):\n",
    "    \"\"\"Converts a list to its length or a list of lists to a list of lengths.\"\"\"\n",
    "    def forward(self, input):\n",
    "        if isinstance(input[0], list) or isinstance(input[0], torch.Tensor):\n",
    "            lengths = []\n",
    "            for text in input:\n",
    "                lengths.append(len(text))\n",
    "            return lengths\n",
    "        elif isinstance(input, list) or isinstance(input, torch.Tensor):\n",
    "            return len(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")\n",
    "\n",
    "class PaddingMask(torch.nn.Module):\n",
    "    \"\"\"Converts a list of padded sequences to a binary mask that shows which tokens are padding.\"\"\"\n",
    "    def __init__(self, padding_value):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "    def _to_mask(self, sequence):\n",
    "        return [0 if token == self.padding_value else 1 for token in sequence]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input[0], list) or isinstance(input[0], torch.Tensor):\n",
    "            return [self._to_mask(seq) for seq in input]\n",
    "        elif isinstance(input, list) or isinstance(input, torch.Tensor):\n",
    "            return self._to_mask(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")\n",
    "\n",
    "\n",
    "input_transform_common = T.Sequential(\n",
    "    SpacyTokenizer(),  # Tokenize\n",
    "    T.VocabTransform(text_vocab),  # Convert to vocab IDs\n",
    "    T.AddToken(token=text_vocab[\"<eos>\"], begin=False),  # Add EOS\n",
    ")\n",
    "\n",
    "input_transform = T.Sequential(\n",
    "    T.ToTensor(padding_value=text_vocab[\"<pad>\"]),  # Convert to tensor and pad\n",
    ")\n",
    "\n",
    "lengths_transform = T.Sequential(\n",
    "    ToLengths(),\n",
    "    T.ToTensor(),\n",
    ")\n",
    "\n",
    "output_transform_common = T.Sequential(\n",
    "    SpacyTokenizer(),  # Tokenize\n",
    "    T.VocabTransform(text_vocab),  # Convert to vocab IDs\n",
    "    T.AddToken(token=text_vocab[\"<eos>\"], begin=False),  # Add EOS\n",
    ")\n",
    "\n",
    "output_transform = T.Sequential(\n",
    "    T.ToTensor(padding_value=text_vocab[\"<pad>\"]),  # Convert to tensor and pad\n",
    ")\n",
    "\n",
    "mask_transform = T.Sequential(\n",
    "    PaddingMask(padding_value=text_vocab[\"<pad>\"]),\n",
    "    T.ToTensor(dtype=bool),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc89e43-9e43-4e51-a4b0-14ceaf6183c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Sort in the batch using reverse order of input length\n",
    "    sort_tokenizer = SpacyTokenizer()\n",
    "    batch.sort(key=lambda x: len(sort_tokenizer(x[0])), reverse=True)\n",
    "\n",
    "    inputs, outputs = zip(*batch)\n",
    "    \n",
    "    # Input processing\n",
    "    inputs = input_transform_common(list(inputs))\n",
    "    lengths = lengths_transform(inputs)\n",
    "    inputs = input_transform(inputs)\n",
    "    \n",
    "    # Output processing\n",
    "    outputs = output_transform_common(list(outputs))\n",
    "    max_output_length = max([len(output)for output in outputs])  # Step 4.1\n",
    "    outputs = output_transform(outputs)\n",
    "    mask = mask_transform(outputs)\n",
    "\n",
    "    # Transpose\n",
    "    inputs = inputs.T\n",
    "    outputs = outputs.T\n",
    "    mask = mask.T\n",
    "\n",
    "    # Ensure boolean dtype for mask due to a torchtext bug.\n",
    "    mask = mask.bool()\n",
    "\n",
    "    return inputs.to(DEVICE), lengths.to(\"cpu\"), outputs.to(DEVICE), mask.to(DEVICE), max_output_length\n",
    "\n",
    "def _get_dataloader(data, batch_size):\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "dataloader = _get_dataloader(final_movie_line_pairs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30855493-676c-41e4-8654-cff24cc02377",
   "metadata": {},
   "source": [
    "We can then inspect the DataLoader and our batches a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb8ec2-ff9f-4b8c-9bb9-1c6ab691cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = _get_dataloader(final_movie_line_pairs, 5)  # Small batch size as a test\n",
    "\n",
    "inputs, lengths, outputs, mask, max_output_length = next(iter(test_dataloader))\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Inputs size: {inputs.size()}\")\n",
    "print(f\"Input Lengths: {lengths}\")\n",
    "print(f\"\\nOutputs: {outputs}\")\n",
    "print(f\"Inputs size: {outputs.size()}\")\n",
    "print(f\"Output Padding Mask: {mask}\")\n",
    "print(f\"Padding mask size: {mask.size()}\")\n",
    "print(f\"Max Output length: {max_output_length}\")\n",
    "print(f\"\\nPadding token value: {text_vocab['<pad>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7855b4d-81af-4c57-bac0-4dfe414fd906",
   "metadata": {},
   "source": [
    "As a sanity check, you may notice that the input lengths correspond to the amount of items in the input sequences that aren't padded (the value isn't the padding token value of `1`). Additionally you may notice that in the output padding mask, the value of `0` does indeed correspond to the items in the `outputs` tensor where the padding token `1` is present. The maximum output length also correctly corresponds to the maximum length of the unpadded output tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768cd75b-6fa2-47f4-b122-ce1a8b8be05d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating our Models\n",
    "The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The goal of a seq2seq model is to take a variable-length sequence as an input, and return a variable-length sequence as an output using a fixed-sized model.\n",
    "\n",
    "[Sutskever et al.](https://arxiv.org/abs/1409.3215) discovered that by using two separate recurrent neural nets together, we can accomplish this task. One RNN acts as an **encoder**, which encodes a variable length input sequence to a fixed-length context vector. In theory, this context vector (the final hidden layer of the RNN) will contain semantic information about the query sentence that is input to the bot. The second RNN is a **decoder**, which takes an input word and the context vector, and returns a guess for the next word in the sequence and a hidden state to use in the next iteration.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq_ts.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd784c4-6275-47a6-8801-67d12ec19ae9",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "As our encoder, we will use a very similar architecture to we saw in previous labs in this module: a Bi-directional Recurrent Neural Network. As our RNN architecture, we will use GRUs.\n",
    "\n",
    "We will of course also have an embedding layer, but unlike previous labs, it won't live in our model. We will want the embeddings to be consistent across our Encoder and Decoder so we will have the embedding layer be a reference to a pre-defined layer that will come in as an argument at Encoder instantiation time.\n",
    "\n",
    "Besides that, the rest is just as in previous labs. To pass our sequences through the RNN, we will use the `pack_padded_sequence` utility along with the lengths we computed in our dataset preparation, and after we have run it through our recurrent model we will unpack it using `pad_packed_sequence`. We will also sum the bidirectional GRU outputs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaa118-3de2-4fdf-9689-4dbe098ad982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs, lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings & pack padded sequences\n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "\n",
    "        # Forward pass through GRU & unpack\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "        # Sum bidirectional GRU outputs & output\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594b757-7ac1-47c5-bd6a-e84990b13f9b",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. It continues generating words until it outputs an EOS token, which if you remember we included in every line in our dataset processing pipelines.\n",
    "\n",
    "A common problem with a vanilla seq2seq decoder is that if we rely solely on the context vector to encode the entire input sequence’s meaning, it is likely that we will have information loss. This is especially the case when dealing with long input sequences, greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, [Bahdanau et al.](https://arxiv.org/abs/1409.0473) created an “attention mechanism” that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current hidden state and the encoder’s outputs. The output attention weights have the same shape as the input sequence, allowing us to multiply them by the encoder outputs, giving us a weighted sum which indicates the parts of encoder output to pay attention to. [Sean Robertson’s](https://github.com/spro) figure describes this very well:\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/attn2.png)\n",
    "\n",
    "[Luong et al.](https://arxiv.org/abs/1508.04025) improved upon Bahdanau et al.’s groundwork by creating “Global attention”. The key difference is that with “Global attention”, we consider all of the encoder’s hidden states, as opposed to Bahdanau et al.’s “Local attention”, which only considers the encoder’s hidden state from the current time step. Another difference is that with “Global attention”, we calculate attention weights, or energies, using the hidden state of the decoder from the current time step only. Bahdanau et al.’s attention calculation requires knowledge of the decoder’s state from the previous time step. Also, Luong et al. provides various methods to calculate the attention energies between the encoder output and decoder output which are called “score functions”:\n",
    "\n",
    "$score(\\mathbf{h}_t,\\bar{\\mathbf{h}}_s)=\\begin{cases}\n",
    "    \\mathbf{h}_t^T\\bar{\\mathbf{h}}_s & \\text{dot}\\\\\n",
    "    \\mathbf{h}_t^T\\mathbf{W}_\\alpha \\bar{\\mathbf{h}}_s & \\text{general}\\\\\n",
    "    \\mathbf{v}_\\alpha^T \\tanh (\\mathbf{W}_\\alpha[\\mathbf{h}_t;\\bar{\\mathbf{h}}_s]) & \\text{concat}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "where $\\mathbf{h}_t$ = current target decoder state and $\\bar{\\mathbf{h}}_s$ = all encoder states.\n",
    "\n",
    "Overall, the Global attention mechanism can be summarized by the following figure. Note that we will implement the “Attention Layer” as a separate `nn.Module` called Attn. The output of this module is a softmax normalized weights tensor of shape *(batch_size, 1, max_length)*.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/global_attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757319f1-8cb0-4ae8-9f4f-0551eb1b796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    \"\"\"Luong attention layer\"\"\"\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        # Different methods to compute attention energy according to the equation above\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':  # Weight matrix W for general attention method\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':  # Weight matrix W and weight vector v for concat attention method\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        \"\"\"Dot method implementation.\"\"\"\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        \"\"\"General method implementation.\"\"\"\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        \"\"\"Concat method implementation.\"\"\"\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"Calculate the attention weights (energies) based on the given method\"\"\"\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "    \n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)  # 1. Get embedding of current word\n",
    "        embedded = self.embedding_dropout(embedded)  # 1.5 Dropout on the embeddings\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)  # 2. Forward through unidirectional GRU\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)  # 3. Calculate attention weights from the current GRU output\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # 4. Multiply attention weights to encoder outputs to get\n",
    "                                                                    # new \"weighted sum\" context vector\n",
    "        rnn_output = rnn_output.squeeze(0)  # 5. Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output) # 6. Predict next word using Luong eq. 6\n",
    "        output = F.softmax(output, dim=1) # 7. Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00355cfa-1351-4eaf-9223-ddb588acaa08",
   "metadata": {},
   "source": [
    "# Training\n",
    "Since we are dealing with batches of padded sequences, we cannot simply consider all elements of the tensor when calculating loss. We define `maskNLLLoss` to calculate our loss based on our decoder’s output tensor, the target tensor, and a binary mask tensor describing the padding of the target tensor. This loss function calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df67eb-3aa7-481a-a6dd-b9a193ce1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(DEVICE)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248268a7",
   "metadata": {},
   "source": [
    "We will use a couple of clever tricks to aid in convergence:\n",
    "\n",
    "1. The first trick is using **teacher forcing**. This means that at some probability, set by `teacher_forcing_ratio`, we use the current target word as the decoder’s next input rather than using the decoder’s current guess. This technique acts as training wheels for the decoder, aiding in more efficient training. However, teacher forcing can lead to model instability during inference, as the decoder may not have a sufficient chance to truly craft its own output sequences during training. Thus, we must be mindful of how we are setting the `teacher_forcing_ratio`, and not be fooled by fast convergence.\n",
    "\n",
    "2. The second trick that we implement is **gradient clipping**. This is a commonly used technique for countering the “exploding gradient” problem. In essence, by clipping or thresholding gradients to a maximum value, we prevent the gradients from growing exponentially and either overflow (`NaN`), or overshoot steep cliffs in the cost function.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/grad_clip.png)\n",
    "\n",
    "Overall, the training process will involve the following sequence:\n",
    "\n",
    "1. Forward pass entire input batch through encoder.\n",
    "2. Initialize decoder inputs as the BOS token, and hidden state as the encoder’s final hidden state.\n",
    "3. Forward the input batch through the decoder one time step at a time.\n",
    "4. If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "5. Calculate and accumulate loss.\n",
    "6. Perform backpropagation.\n",
    "7. Clip gradients.\n",
    "8. Update encoder and decoder model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    inputs, lengths,  # Input\n",
    "    outputs, mask, max_target_len,  # Output\n",
    "    encoder, decoder,  # Models\n",
    "    encoder_optimizer, decoder_optimizer,  # Optimizers\n",
    "    clip, teacher_forcing_ratio  # Hyper parameters.\n",
    "):\n",
    "    \"\"\"Train function to be run for a single data point.\"\"\"\n",
    "    # Implicit batch size\n",
    "    batch_size = inputs.size()[1]\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Lengths for rnn packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss, print_losses, n_totals = 0, [], 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(inputs, lengths)\n",
    "\n",
    "    # Create initial decoder input & Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_input = torch.LongTensor([[text_vocab[\"<bos>\"] for _ in range(batch_size)]]).to(DEVICE)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # HACK Teacher Forcing - Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        if use_teacher_forcing:  # HACK Teacher forcing: next input is current target\n",
    "            decoder_input = outputs[t].view(1, -1)\n",
    "        else:  # next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]).to(DEVICE)\n",
    "\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = maskNLLLoss(decoder_output, outputs[t], mask[t])\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # HACK Gradient clipping: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1262c",
   "metadata": {},
   "source": [
    "However, that is for a single data point only. We still need to define a function that will run our training procedure for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def train_epoch(\n",
    "    iterator,\n",
    "    encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "    clip, teacher_forcing_ratio\n",
    "):\n",
    "    \"\"\"Training procedure for an epoch.\"\"\"\n",
    "    loss_sum, batches = 0, 0\n",
    "\n",
    "    # Training loop \n",
    "    for batch in tqdm(iterator, desc=\"\\tTraining\"):\n",
    "        # Extract fields from batch\n",
    "        inputs, lengths, outputs, mask, max_target_len = batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train_step(\n",
    "            inputs, lengths, outputs, mask, max_target_len,\n",
    "            encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "            clip, teacher_forcing_ratio\n",
    "        )\n",
    "        loss_sum += loss\n",
    "        batches += 1\n",
    "    \n",
    "    return loss_sum / batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8681828",
   "metadata": {},
   "source": [
    "Last but not least, we will need a function for the entire training proecss (all epochs). We will also be saving our model periodically on best training loss (so every epoch in practice) using PyTorch's `save()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, epochs, encoder, decoder, encoder_optimizer, decoder_optimizer, clip, teacher_forcing_ratio, encoder_n_layers, decoder_n_layers, hidden_size, attn_model):\n",
    "    \"\"\"Training procedure for the entire model (multiple epochs).\"\"\"\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "\n",
    "        train_loss = train_epoch(iterator, encoder, decoder, encoder_optimizer, decoder_optimizer, clip, teacher_forcing_ratio)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save({\n",
    "                \"encoder\": encoder.state_dict(),\n",
    "                \"decoder\": decoder.state_dict(),\n",
    "                \"embedding\": encoder.embedding.state_dict(),\n",
    "                \"loss\": train_loss,\n",
    "                \"vocabulary\": text_vocab,\n",
    "                \"encoder_n_layers\": encoder_n_layers,\n",
    "                \"decoder_n_layers\": decoder_n_layers,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"attn_model\": attn_model,\n",
    "            }, f\"chatbot-{encoder_n_layers}-{decoder_n_layers}-{hidden_size}-{attn_model}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed356c1",
   "metadata": {},
   "source": [
    "Now we can finally train our model. Feel free to play with the hyperparameters for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Model parameters\n",
    "HIDDEN_SIZE = 500\n",
    "ENCODER_N_LAYERS = 2\n",
    "DECODER_N_LAYERS = 2\n",
    "ATTENTION_MODEL = \"dot\"\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training length parameters\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "ITERATOR = _get_dataloader(final_movie_line_pairs, BATCH_SIZE)\n",
    "\n",
    "# Training hyper parameters\n",
    "CLIP = 50.0\n",
    "TEACHER_FORCING_RATIO = 1.0\n",
    "LEARNING_RATE = 0.0001\n",
    "DECODER_LEARNING_RATIO = 5.0\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=HIDDEN_SIZE)\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    embedding=embedding, \n",
    "    n_layers=ENCODER_N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = LuongAttnDecoderRNN(\n",
    "    attn_model=ATTENTION_MODEL,\n",
    "    embedding=embedding,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=len(text_vocab),\n",
    "    n_layers=DECODER_N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE * DECODER_LEARNING_RATIO)\n",
    "\n",
    "# Run training!\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'} for training.\")\n",
    "train(\n",
    "    ITERATOR, N_EPOCHS,\n",
    "    encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "    CLIP, TEACHER_FORCING_RATIO,\n",
    "    ENCODER_N_LAYERS, DECODER_N_LAYERS, HIDDEN_SIZE, ATTENTION_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55cdb0d",
   "metadata": {},
   "source": [
    "# Inference\n",
    "In order to interact with the chatbot, we need to be able to convert its decoder's output from numbers to actual words. To do that, we will use Greedy decoding, which is essentially what we're already doing in our training process when we're not using teacher forcing. Each time step, we'll choose the word from the decoder output with the highest softmax value. This decoding method is optimal on a single time-step level.\n",
    "\n",
    "To facilitate the greedy decoding operation, we'll define a `GreedySearchDecoder` class. When run, an object of this class takes an input sequence, a scalar input length tensor and a maximum length to bound the response sentence length.\n",
    "\n",
    "The entire process will work as follows:\n",
    "1. Forward input through the encoder.\n",
    "2. Prepare the encoder's final hidden layer to be the first hidden input to the decoder.\n",
    "3. Initialize the decoder's first input as the BOS token.\n",
    "4. Initialize tensors to append decoded words to.\n",
    "5. Iteratively decode one word at a time:\n",
    "    1. Forward pass through decoder.\n",
    "    2. Obtain most likely word token and its softmax score.\n",
    "    3. Record token and score.\n",
    "    4. Prepare current token to be next decoder input.\n",
    "6. Return a collection of word tokens and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f40901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder & Initialize decoder input with SOS_token\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        decoder_input = torch.ones(1, 1, device=DEVICE, dtype=torch.long) * self.vocab[\"<bos>\"]\n",
    "\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=DEVICE, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=DEVICE)\n",
    "\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder & Obtain most likely word token and its softmax score\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af0d7d",
   "metadata": {},
   "source": [
    "Finally, we'll make a utility class called `Chatbot` that will provide an interface for interaction. It will also handle tokenization and pre-processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot():\n",
    "    def __init__(self, model_file, device, max_length=10):\n",
    "        self.model = torch.load(model_file)\n",
    "        self.vocab = self.model[\"vocabulary\"]\n",
    "        self.hidden_size = self.model[\"hidden_size\"]\n",
    "        self.attn_model = self.model[\"attn_model\"]\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=HIDDEN_SIZE)\n",
    "        self.embedding.load_state_dict(self.model[\"embedding\"])\n",
    "        \n",
    "        self.encoder = EncoderRNN(self.hidden_size, self.embedding, self.model[\"encoder_n_layers\"]).to(self.device)\n",
    "        self.encoder.load_state_dict(self.model[\"encoder\"])\n",
    "\n",
    "        self.decoder = LuongAttnDecoderRNN(\n",
    "            self.attn_model, self.embedding, self.hidden_size, len(self.vocab), self.model[\"decoder_n_layers\"]\n",
    "        ).to(self.device)\n",
    "        self.decoder.load_state_dict(self.model[\"decoder\"])\n",
    "\n",
    "        self.searcher = GreedySearchDecoder(self.encoder, self.decoder, self.vocab)\n",
    "    \n",
    "    def _transform_input(self, input):\n",
    "        input = input_transform_common(input)\n",
    "        lengths = lengths_transform(input)\n",
    "        input = input_transform(input)\n",
    "        input = input.T\n",
    "        return input.to(DEVICE), lengths.cpu()\n",
    "    \n",
    "    def answer(self, question):\n",
    "        processed_question, length = self._transform_input([question])\n",
    "        _answer, scores = self.searcher(processed_question, length, self.max_length)\n",
    "        decoded_answer = [self.vocab.get_itos()[word.item()] for word in _answer]\n",
    "        decoded_answer[:] = [word for word in decoded_answer if not (word == \"<eos>\" or word == \"<pad>\")]\n",
    "        return \" \".join(decoded_answer)\n",
    "\n",
    "    def chat(self):\n",
    "        while(1):\n",
    "            try:\n",
    "                question = input(\"> \")\n",
    "                if question in [\"q\", \"quit\"]:\n",
    "                    break\n",
    "                \n",
    "                _answer = self.answer(question)\n",
    "                print(\"Bot: \", _answer)\n",
    "            except KeyError:\n",
    "                print(\"Error: Encountered unknown word.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134da66",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e270049",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(\"chatbot-2-2-500-dot.pt\", DEVICE)\n",
    "bot.chat()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "920e794a5b259f1fa91519fb28c3de4dd3360c2bef803c48a0e90eeb944716d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
